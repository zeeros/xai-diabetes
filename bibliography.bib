@article{abbasEvaluatingAutomatedMachine2022,
  title = {Evaluating an Automated Machine Learning Model That Predicts Visual Acuity Outcomes in Patients with Neovascular Age-Related Macular Degeneration},
  author = {Abbas, Abdallah and O'Byrne, Ciara and Fu, Dun Jack and Moraes, Gabriella and Balaskas, Konstantinos and Struyven, Robbert and Beqiri, Sara and Wagner, Siegfried K. and Korot, Edward and Keane, Pearse A.},
  year = {2022},
  month = feb,
  journal = {Graefe's archive for clinical and experimental ophthalmology = Albrecht von Graefes Archiv f\"ur klinische und experimentelle Ophthalmologie},
  issn = {1435-702X},
  doi = {10.1007/s00417-021-05544-y},
  abstract = {Neovascular age-related macular degeneration (nAMD) is a major global cause of blindness. Whilst anti-vascular endothelial growth factor (anti-VEGF) treatment is effective, response varies considerably between individuals. Thus, patients face substantial uncertainty regarding their future ability to perform daily tasks. In this study, we evaluate the performance of an automated machine learning (AutoML) model which predicts visual acuity (VA) outcomes in patients receiving treatment for nAMD, in comparison to a manually coded model built using the same dataset. Furthermore, we evaluate model performance across ethnic groups and analyse how the models reach their predictions.},
  langid = {english},
  keywords = {Anti-VEGF,Artificial intelligence,Automated machine learning,Model interpretability,Neovascular age-related macular degeneration,OCT}
}

@article{abdullahReviewInterpretableML2021,
  title = {A {{Review}} of {{Interpretable ML}} in {{Healthcare}}: {{Taxonomy}}, {{Applications}}, {{Challenges}}, and {{Future Directions}}},
  shorttitle = {A {{Review}} of {{Interpretable ML}} in {{Healthcare}}},
  author = {Abdullah, Talal A. A. and Zahid, Mohd Soperi Mohd and Ali, Waleed},
  year = {2021},
  month = dec,
  journal = {Symmetry},
  volume = {13},
  number = {12},
  pages = {2439},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-8994},
  doi = {10.3390/sym13122439},
  abstract = {We have witnessed the impact of ML in disease diagnosis, image recognition and classification, and many more related fields. Healthcare is a sensitive field related to people's lives in which decisions need to be carefully taken based on solid evidence. However, most ML models are complex, i.e., black-box, meaning they do not provide insights into how the problems are solved or why such decisions are proposed. This lack of interpretability is the main reason why some ML models are not widely used yet in real environments such as healthcare. Therefore, it would be beneficial if ML models could provide explanations allowing physicians to make data-driven decisions that lead to higher quality service. Recently, several efforts have been made in proposing interpretable machine learning models to become more convenient and applicable in real environments. This paper aims to provide a comprehensive survey and symmetry phenomena of IML models and their applications in healthcare. The fundamental characteristics, theoretical underpinnings needed to develop IML, and taxonomy for IML are presented. Several examples of how they are applied in healthcare are investigated to encourage and facilitate the use of IML models in healthcare. Furthermore, current limitations, challenges, and future directions that might impact applying ML in healthcare are addressed.},
  langid = {english},
  keywords = {applications,challenges,healthcare,interpretability,machine learning,taxonomy}
}

@article{albertiDefinitionDiagnosisClassification1998,
  title = {Definition, Diagnosis and Classification of Diabetes Mellitus and Its Complications. {{Part}} 1: Diagnosis and Classification of Diabetes Mellitus. {{Provisional}} Report of a {{WHO Consultation}}},
  shorttitle = {Definition, Diagnosis and Classification of Diabetes Mellitus and Its Complications. {{Part}} 1},
  author = {Alberti, K.g.m.m. and Zimmet, P.z.},
  year = {1998},
  journal = {Diabetic Medicine},
  volume = {15},
  number = {7},
  pages = {539--553},
  issn = {1096-9136},
  doi = {10.1002/(SICI)1096-9136(199807)15:7<539::AID-DIA668>3.0.CO;2-S},
  abstract = {The classification of diabetes mellitus and the tests used for its diagnosis were brought into order by the National Diabetes Data Group of the USA and the second World Health Organization Expert Committee on Diabetes Mellitus in 1979 and 1980. Apart from minor modifications by WHO in 1985, little has been changed since that time. There is however considerable new knowledge regarding the aetiology of different forms of diabetes as well as more information on the predictive value of different blood glucose values for the complications of diabetes. A WHO Consultation has therefore taken place in parallel with a report by an American Diabetes Association Expert Committee to re-examine diagnostic criteria and classification. The present document includes the conclusions of the former and is intended for wide distribution and discussion before final proposals are submitted to WHO for approval. The main changes proposed are as follows. The diagnostic fasting plasma (blood) glucose value has been lowered to {$\geq$}7.0 mmol l-1 (6.1 mmol l-1). Impaired Glucose Tolerance (IGT) is changed to allow for the new fasting level. A new category of Impaired Fasting Glycaemia (IFG) is proposed to encompass values which are above normal but below the diagnostic cut-off for diabetes (plasma {$\geq$}6.1 to {$<$}7.0 mmol l-1; whole blood {$\geq$}5.6 to {$<$}6.1 mmol l-1). Gestational Diabetes Mellitus (GDM) now includes gestational impaired glucose tolerance as well as the previous GDM. The classification defines both process and stage of the disease. The processes include Type 1, autoimmune and non-autoimmune, with beta-cell destruction; Type 2 with varying degrees of insulin resistance and insulin hyposecretion; Gestational Diabetes Mellitus; and Other Types where the cause is known (e.g. MODY, endocrinopathies). It is anticipated that this group will expand as causes of Type 2 become known. Stages range from normoglycaemia to insulin required for survival. It is hoped that the new classification will allow better classification of individuals and lead to fewer therapeutic misjudgements. \textcopyright{} 1998 WHO},
  langid = {english},
  keywords = {classification,diabetes mellitus,diagnosis,gestational diabetes mellitus,Type 1 diabetes,Type 2 diabetes},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291096-9136\%28199807\%2915\%3A7\%3C539\%3A\%3AAID-DIA668\%3E3.0.CO\%3B2-S}
}

@article{americandiabetesassociationDiagnosisClassificationDiabetes2013,
  title = {Diagnosis and {{Classification}} of {{Diabetes Mellitus}}},
  author = {{American Diabetes Association}},
  year = {2013},
  month = dec,
  journal = {Diabetes Care},
  volume = {37},
  number = {Supplement\_1},
  pages = {S81-S90},
  issn = {0149-5992},
  doi = {10.2337/dc14-S081}
}

@article{amritphalePredictors30DayUnplanned2021,
  title = {Predictors of 30-{{Day Unplanned Readmission After Carotid Artery Stenting Using Artificial Intelligence}}},
  author = {Amritphale, Amod and Chatterjee, Ranojoy and Chatterjee, Suvo and Amritphale, Nupur and Rahnavard, Ali and Awan, G. Mustafa and Omar, Bassam and Fonarow, Gregg C.},
  year = {2021},
  month = jun,
  journal = {Advances in therapy},
  volume = {38},
  number = {6},
  pages = {2954--2972},
  issn = {1865-8652},
  doi = {10.1007/s12325-021-01709-7},
  abstract = {This study aimed to describe the rates and causes of unplanned readmissions within 30 days following carotid artery stenting (CAS) and to use artificial intelligence machine learning analysis for creating a prediction model for short-term readmissions. The prediction of unplanned readmissions after index CAS remains challenging. There is a need to leverage deep machine learning algorithms in order to develop robust prediction tools for early readmissions.},
  langid = {english},
  keywords = {Artificial intelligence,Carotid artery stenting,Machine learning,Readmission}
}

@misc{apleyVisualizingEffectsPredictor2019,
  title = {Visualizing the {{Effects}} of {{Predictor Variables}} in {{Black Box Supervised Learning Models}}},
  author = {Apley, Daniel W. and Zhu, Jingyu},
  year = {2019},
  month = aug,
  number = {arXiv:1612.08468},
  eprint = {1612.08468},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1612.08468},
  abstract = {When fitting black box supervised learning models (e.g., complex trees, neural networks, boosted trees, random forests, nearest neighbors, local kernel-weighted methods, etc.), visualizing the main effects of the individual predictor variables and their low-order interaction effects is often important, and partial dependence (PD) plots are the most popular approach for accomplishing this. However, PD plots involve a serious pitfall if the predictor variables are far from independent, which is quite common with large observational data sets. Namely, PD plots require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data, which can render the PD plots unreliable. Although marginal plots (M plots) do not require such extrapolation, they produce substantially biased and misleading results when the predictors are dependent, analogous to the omitted variable bias in regression. We present a new visualization approach that we term accumulated local effects (ALE) plots, which inherits the desirable characteristics of PD and M plots, without inheriting their preceding shortcomings. Like M plots, ALE plots do not require extrapolation; and like PD plots, they are not biased by the omitted variable phenomenon. Moreover, ALE plots are far less computationally expensive than PD plots.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology}
}

@misc{aryaOneExplanationDoes2019,
  title = {One {{Explanation Does Not Fit All}}: {{A Toolkit}} and {{Taxonomy}} of {{AI Explainability Techniques}}},
  shorttitle = {One {{Explanation Does Not Fit All}}},
  author = {Arya, Vijay and Bellamy, Rachel K. E. and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Liao, Q. Vera and Luss, Ronny and Mojsilovi{\'c}, Aleksandra and Mourad, Sami and Pedemonte, Pablo and Raghavendra, Ramya and Richards, John and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Singh, Moninder and Varshney, Kush R. and Wei, Dennis and Zhang, Yunfeng},
  year = {2019},
  month = sep,
  number = {arXiv:1909.03012},
  eprint = {1909.03012},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.03012},
  abstract = {As artificial intelligence and machine learning algorithms make further inroads into society, calls are increasing from multiple stakeholders for these algorithms to explain their outputs. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, present different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360 (http://aix360.mybluemix.net/), an open-source software toolkit featuring eight diverse and state-of-the-art explainability methods and two evaluation metrics. Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. We also discuss enhancements to bring research innovations closer to consumers of explanations, ranging from simplified, more accessible versions of algorithms, to tutorials and an interactive web demo to introduce AI explainability to different audiences and application domains. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Statistics - Machine Learning}
}

@inproceedings{athanasiouExplainableXGBoostBased2020,
  title = {An Explainable {{XGBoost}}\textendash{{Based}} Approach towards Assessing the Risk of Cardiovascular Disease in Patients with {{Type}} 2 {{Diabetes Mellitus}}},
  booktitle = {2020 {{IEEE}} 20th {{International Conference}} on {{Bioinformatics}} and {{Bioengineering}} ({{BIBE}})},
  author = {Athanasiou, Maria and Sfrintzeri, Konstantina and Zarkogianni, Konstantia and Thanopoulou, Anastasia C. and Nikita, Konstantina S.},
  year = {2020},
  month = oct,
  pages = {859--864},
  issn = {2471-7819},
  doi = {10.1109/BIBE50027.2020.00146},
  abstract = {Cardiovascular Disease (CVD) is an important cause of disability and death among individuals with Diabetes Mellitus (DM). International clinical guidelines for the management of Type 2 DM (T2DM) are founded on primary and secondary prevention and favor the evaluation of CVD-related risk factors towards appropriate treatment initiation. CVD risk prediction models can provide valuable tools for optimizing the frequency of medical visits and performing timely preventive and therapeutic interventions against CVD events. The integration of explainability modalities in these models can enhance human understanding on the reasoning process, maximize transparency and embellish trust towards the models' adoption in clinical practice. The aim of the present study is to develop and evaluate an explainable personalized risk prediction model for the fatal or non-fatal CVD incidence in T2DM individuals. An explainable approach based on the eXtreme Gradient Boosting (XGBoost) and the Tree SHAP (SHapley Additive exPlanations) method is deployed for the calculation of the 5-year CVD risk and the generation of individual explanations on the model's decisions. Data from the 5-year follow up of 560 patients with T2DM are used for development and evaluation purposes. The obtained results (AUC=71.13\%) indicate the potential of the proposed approach to handle the unbalanced nature of the used dataset, while providing clinically meaningful insights about the model's decision process.},
  eventtitle = {2020 {{IEEE}} 20th {{International Conference}} on {{Bioinformatics}} and {{Bioengineering}} ({{BIBE}})},
  keywords = {Cardiovascular Disease,Cardiovascular diseases,Cognition,Diabetes,explainability,interpretability,machine learning,Prediction algorithms,Predictive models,Time-frequency analysis,Tools,unbalanced data}
}

@misc{BabelLanguages,
  title = {Babel: {{Languages}}},
  howpublished = {https://orgmode.org/worg/org-contrib/babel/languages/index.html}
}

@inproceedings{bacarditIntersectionEvolutionaryComputation2022,
  title = {The Intersection of Evolutionary Computation and Explainable {{AI}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Bacardit, Jaume and Brownlee, Alexander E. I. and Cagnoni, Stefano and Iacca, Giovanni and McCall, John and Walker, David},
  year = {2022},
  month = jul,
  series = {{{GECCO}} '22},
  pages = {1757--1762},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3520304.3533974},
  abstract = {In the past decade, Explainable Artificial Intelligence (XAI) has attracted a great interest in the research community, motivated by the need for explanations in critical AI applications. Some recent advances in XAI are based on Evolutionary Computation (EC) techniques, such as Genetic Programming. We call this trend EC for XAI. We argue that the full potential of EC methods has not been fully exploited yet in XAI, and call the community for future efforts in this field. Likewise, we find that there is a growing concern in EC regarding the explanation of population-based methods, i.e., their search process and outcomes. While some attempts have been done in this direction (although, in most cases, those are not explicitly put in the context of XAI), we believe that there are still several research opportunities and open research questions that, in principle, may promote a safer and broader adoption of EC in real-world applications. We call this trend XAI within EC. In this position paper, we briefly overview the main results in the two above trends, and suggest that the EC community may play a major role in the achievement of XAI.},
  isbn = {978-1-4503-9268-6},
  keywords = {evolutionary computation,explainable artificial intelligence,machine learning,optimization}
}

@article{banieckiDalexResponsibleMachine2021,
  title = {Dalex: {{Responsible Machine Learning}} with {{Interactive Explainability}} and {{Fairness}} in {{Python}}},
  shorttitle = {Dalex},
  author = {Baniecki, Hubert and Kretowicz, Wojciech and Pi{\k{a}}tyszek, Piotr and Wi{\'s}niewski, Jakub and Biecek, Przemys{\l}aw},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {214},
  pages = {1--7},
  issn = {1533-7928},
  abstract = {In modern machine learning, we observe the phenomenon of opaqueness debt, which manifests itself by an increased risk of discrimination, lack of reproducibility, and deflated performance due to data drift. An increasing amount of available data and computing power results in the growing complexity of black-box predictive models. To manage these issues, good MLOps practice asks for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity for deeper model transparency comes from both scientific and social domains and is also caused by emerging laws and regulations on artificial intelligence. To facilitate the responsible development of machine learning models, we introduce dalex, a Python package which implements a model-agnostic interface for interactive explainability and fairness. It adopts the design crafted through the development of various tools for explainable machine learning; thus, it aims at the unification of existing solutions. This library's source code and documentation are available under open license at https://python.drwhy.ai.}
}

@misc{bosschieterUsingInterpretableMachine2022,
  title = {Using {{Interpretable Machine Learning}} to {{Predict Maternal}} and {{Fetal Outcomes}}},
  author = {Bosschieter, Tomas M. and Xu, Zifei and Lan, Hui and Lengerich, Benjamin J. and Nori, Harsha and Sitcov, Kristin and Souter, Vivienne and Caruana, Rich},
  year = {2022},
  month = jul,
  number = {arXiv:2207.05322},
  eprint = {2207.05322},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.05322},
  abstract = {Most pregnancies and births result in a good outcome, but complications are not uncommon and when they do occur, they can be associated with serious implications for mothers and babies. Predictive modeling has the potential to improve outcomes through better understanding of risk factors, heightened surveillance, and more timely and appropriate interventions, thereby helping obstetricians deliver better care. For three types of complications we identify and study the most important risk factors using Explainable Boosting Machine (EBM), a glass box model, in order to gain intelligibility: (i) Severe Maternal Morbidity (SMM), (ii) shoulder dystocia, and (iii) preterm preeclampsia. While using the interpretability of EBM's to reveal surprising insights into the features contributing to risk, our experiments show EBMs match the accuracy of other black-box ML methods such as deep neural nets and random forests.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications}
}

@inproceedings{chanderEvaluatingExplanationsCognitive2018,
  title = {Evaluating {{Explanations}} by {{Cognitive Value}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  author = {Chander, Ajay and Srinivasan, Ramya},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {314--328},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-99740-7_23},
  abstract = {The transparent AI initiative has ignited several academic and industrial endeavors and produced some impressive technologies and results thus far. Many state-of-the-art methods provide explanations that mostly target the needs of AI engineers. However, there is very little work on providing explanations that support the needs of business owners, software developers, and consumers who all play significant roles in the service development and use cycle. By considering the overall context in which an explanation is presented, including the role played by the human-in-the-loop, we can hope to craft effective explanations. In this paper, we introduce the notion of the ``cognitive value'' of an explanation and describe its role in providing effective explanations within a given context. Specifically, we consider the scenario of a business owner seeking to improve sales of their product, and compare explanations provided by some existing interpretable machine learning algorithms (random forests, scalable Bayesian Rules, causal models) in terms of the cognitive value they offer to the business owner. We hope that our work will foster future research in the field of transparent AI to incorporate the cognitive value of explanations in crafting and evaluating explanations.},
  isbn = {978-3-319-99740-7},
  langid = {english},
  keywords = {AI,Business owner,Causal modeling,Cognitive value,Explanations}
}

@article{chenPredictingInpatientClinical2017,
  title = {Predicting Inpatient Clinical Order Patterns with Probabilistic Topic Models vs Conventional Order Sets},
  author = {Chen, Jonathan H. and Goldstein, Mary K. and Asch, Steven M. and Mackey, Lester and Altman, Russ B.},
  year = {2017},
  month = may,
  journal = {Journal of the American Medical Informatics Association: JAMIA},
  volume = {24},
  number = {3},
  pages = {472--480},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocw136},
  abstract = {OBJECTIVE: Build probabilistic topic model representations of hospital admissions processes and compare the ability of such models to predict clinical order patterns as compared to preconstructed order sets. MATERIALS AND METHODS: The authors evaluated the first 24 hours of structured electronic health record data for {$>$}\,10\,K inpatients. Drawing an analogy between structured items (e.g., clinical orders) to words in a text document, the authors performed latent Dirichlet allocation probabilistic topic modeling. These topic models use initial clinical information to predict clinical orders for a separate validation set of {$>$}\,4\,K patients. The authors evaluated these topic model-based predictions vs existing human-authored order sets by area under the receiver operating characteristic curve, precision, and recall for subsequent clinical orders. RESULTS: Existing order sets predict clinical orders used within 24 hours with area under the receiver operating characteristic curve 0.81, precision 16\%, and recall 35\%. This can be improved to 0.90, 24\%, and 47\% ( P \,{$<$}\,10 -20 ) by using probabilistic topic models to summarize clinical data into up to 32 topics. Many of these latent topics yield natural clinical interpretations (e.g., "critical care," "pneumonia," "neurologic evaluation"). DISCUSSION: Existing order sets tend to provide nonspecific, process-oriented aid, with usability limitations impairing more precise, patient-focused support. Algorithmic summarization has the potential to breach this usability barrier by automatically inferring patient context, but with potential tradeoffs in interpretability. CONCLUSION: Probabilistic topic modeling provides an automated approach to detect thematic trends in patient care and generate decision support content. A potential use case finds related clinical orders for decision support.},
  langid = {english},
  pmcid = {PMC5391730},
  pmid = {27655861},
  keywords = {Algorithms,clinical decision support systems,clinical summarization,data mining,Data Mining,Decision Support Systems; Clinical,Diagnostic Tests; Routine,electronic health records,Electronic Health Records,Hospitalization,Humans,Medical Order Entry Systems,Models; Statistical,order sets,Patient Care,probabilistic topic modeling,ROC Curve}
}

@article{chiccoAdvantagesMatthewsCorrelation2020,
  title = {The Advantages of the {{Matthews}} Correlation Coefficient ({{MCC}}) over {{F1}} Score and Accuracy in Binary Classification Evaluation},
  author = {Chicco, Davide and Jurman, Giuseppe},
  year = {2020},
  month = dec,
  journal = {BMC Genomics},
  volume = {21},
  number = {1},
  pages = {6},
  issn = {1471-2164},
  doi = {10.1186/s12864-019-6413-7},
  abstract = {Abstract Background To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F 1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets. Results The Matthews correlation coefficient (MCC), instead, is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset. Conclusions In this article, we show how MCC produces a more informative and truthful score in evaluating binary classifications than accuracy and F 1 score, by first explaining the mathematical properties, and then the asset of MCC in six synthetic use cases and in a real genomics scenario. We believe that the Matthews correlation coefficient should be preferred to accuracy and F 1 score in evaluating binary classification tasks by all scientific communities.},
  langid = {english}
}

@misc{daveExplainableAIMeets2020,
  title = {Explainable {{AI}} Meets {{Healthcare}}: {{A Study}} on {{Heart Disease Dataset}}},
  shorttitle = {Explainable {{AI}} Meets {{Healthcare}}},
  author = {Dave, Devam and Naik, Het and Singhal, Smiti and Patel, Pankesh},
  year = {2020},
  month = nov,
  number = {arXiv:2011.03195},
  eprint = {2011.03195},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.03195},
  abstract = {With the increasing availability of structured and unstructured data and the swift progress of analytical techniques, Artificial Intelligence (AI) is bringing a revolution to the healthcare industry. With the increasingly indispensable role of AI in healthcare, there are growing concerns over the lack of transparency and explainability in addition to potential bias encountered by predictions of the model. This is where Explainable Artificial Intelligence (XAI) comes into the picture. XAI increases the trust placed in an AI system by medical practitioners as well as AI researchers, and thus, eventually, leads to an increasingly widespread deployment of AI in healthcare. In this paper, we present different interpretability techniques. The aim is to enlighten practitioners on the understandability and interpretability of explainable AI systems using a variety of techniques available which can be very advantageous in the health-care domain. Medical diagnosis model is responsible for human life and we need to be confident enough to treat a patient as instructed by a black-box model. Our paper contains examples based on the heart disease dataset and elucidates on how the explainability techniques should be preferred to create trustworthiness while using AI systems in healthcare.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{Docker,
  title = {Docker},
  shorttitle = {Docker},
  abstract = {Docker is a platform designed to help developers build, share, and run modern applications. We handle the tedious setup, so you can focus on the code.},
  howpublished = {https://www.docker.com/},
  langid = {american}
}

@misc{doshi-velezRigorousScienceInterpretable2017,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = {2017},
  month = mar,
  number = {arXiv:1702.08608},
  eprint = {1702.08608},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1702.08608},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{duellComparisonExplanationsGiven2021,
  title = {A {{Comparison}} of {{Explanations Given}} by {{Explainable Artificial Intelligence Methods}} on {{Analysing Electronic Health Records}}},
  booktitle = {2021 {{IEEE EMBS International Conference}} on {{Biomedical}} and {{Health Informatics}} ({{BHI}})},
  author = {Duell, Jamie and Fan, Xiuyi and Burnett, Bruce and Aarts, Gert and Zhou, Shang-Ming},
  year = {2021},
  month = jul,
  pages = {1--4},
  issn = {2641-3604},
  doi = {10.1109/BHI50953.2021.9508618},
  abstract = {eXplainable Artificial Intelligence (XAI) aims to provide intelligible explanations to users. XAI algorithms such as SHAP, LIME and Scoped Rules compute feature importance for machine learning predictions. Although XAI has attracted much research attention, applying XAI techniques in healthcare to inform clinical decision making is challenging. In this paper, we provide a comparison of explanations given by XAI methods as a tertiary extension in analysing complex Electronic Health Records (EHRs). With a large-scale EHR dataset, we compare features of EHRs in terms of their prediction importance estimated by XAI models. Our experimental results show that the studied XAI methods circumstantially generate different top features; their aberrations in shared feature importance merit further exploration from domain-experts to evaluate human trust towards XAI.},
  eventtitle = {2021 {{IEEE EMBS International Conference}} on {{Biomedical}} and {{Health Informatics}} ({{BHI}})},
  keywords = {Biological system modeling,Black-box,Conferences,Decision making,Electronic Health Records,Explainable AI,Glass-box,Machine learning,Machine Learning,Machine learning algorithms,Medical services,Predictive models}
}

@misc{EthicsGovernanceArtificial,
  title = {Ethics and Governance of Artificial Intelligence for Health - {{https://www.who.int/}}},
  abstract = {WHO guidance},
  langid = {english}
}

@misc{ExplainableBoostingMachine,
  title = {Explainable {{Boosting Machine}} - {{InterpretML}}},
  howpublished = {https://interpret.ml/docs/ebm.html}
}

@misc{ExplainerdashboardExplainerdashboardDocumentation,
  title = {Explainerdashboard \textemdash{} Explainerdashboard 0.2 Documentation}
}

@article{fengIntelligibleModelsHealthCare2021,
  title = {Intelligible {{Models}} for {{HealthCare}}: {{Predicting}} the {{Probability}} of 6-{{Month Unfavorable Outcome}} in {{Patients}} with {{Ischemic Stroke}}},
  shorttitle = {Intelligible {{Models}} for {{HealthCare}}},
  author = {Feng, Xiaobing and Hua, Yingrong and Zou, Jianjun and Jia, Shuopeng and Ji, Jiatong and Xing, Yan and Zhou, Junshan and Liao, Jun},
  year = {2021},
  month = aug,
  journal = {Neuroinform},
  issn = {1559-0089},
  doi = {10.1007/s12021-021-09535-6},
  abstract = {Early prediction of unfavorable outcome after ischemic stroke is significant for clinical management. Machine learning as a novel computational modeling technique could help clinicians to address the challenge. We aim to investigate the applicability of machine learning models for individualized prediction in ischemic stroke patients and demonstrate the utility of various model-agnostic explanation techniques for machine learning predictions. A total of 499 consecutive patients with Unfavorable [modified Rankin Scale (mRS) score 3\textendash 6, n\,=\,140] and favorable (mRS score 0\textendash 2, n\,=\,359) outcome after 6-month from ischemic stroke were enrolled in this study. Four machine learning models, including Random Forest [RF], eXtreme Gradient Boosting [XGBoost], Adaptive Boosting [Adaboost] and Support Vector Machine [SVM] were performed with the area-under-the-curve (AUC): (90.20\,{$\pm$}\,0.22)\%, (86.91\,{$\pm$}\,1.05)\%, (86.49\,{$\pm$}\,2.35)\%, (81.89\,{$\pm$}\,2.40)\%, respectively. Three global interpretability techniques (Feature Importance shows the contribution of selected features, Partial Dependence Plot aims to visualize the average effect of a feature on the predicted probability of unfavorable outcome, Feature Interaction detects the change in the prediction that occurs by varying the features after considering the individual feature effects) and one local interpretability technique (Shapley Value indicates the probability of unfavorable outcome of different instances) have been applied to present the interpretability techniques via visualization. Thereby, the current study is important for better understanding intelligible healthcare analytics via explanations for the prediction of local and global levels, and potentially reduction of the mortality of patients with ischemic stroke by assisting clinicians in the decision-making process.},
  langid = {english},
  keywords = {Interpretability,Ischemic stroke,Machine learning,Unfavorable outcome,Visualization}
}

@article{ghorbaniDeepLearningInterpretation2020,
  title = {Deep Learning Interpretation of Echocardiograms},
  author = {Ghorbani, Amirata and Ouyang, David and Abid, Abubakar and He, Bryan and Chen, Jonathan H. and Harrington, Robert A. and Liang, David H. and Ashley, Euan A. and Zou, James Y.},
  year = {2020},
  month = jan,
  journal = {npj Digit. Med.},
  volume = {3},
  number = {1},
  pages = {1--10},
  publisher = {{Nature Publishing Group}},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0216-8},
  abstract = {Echocardiography uses ultrasound technology to capture high temporal and spatial resolution images of the heart and surrounding structures, and is the most common imaging modality in cardiovascular medicine. Using convolutional neural networks on a large new dataset, we show that deep learning applied to echocardiography can identify local cardiac structures, estimate cardiac function, and predict systemic phenotypes that modify cardiovascular risk but not readily identifiable to human interpretation. Our deep learning model, EchoNet, accurately identified the presence of pacemaker leads (AUC\,=\,0.89), enlarged left atrium (AUC\,=\,0.86), left ventricular hypertrophy (AUC\,=\,0.75), left ventricular end systolic and diastolic volumes (\$\$\{R\}\^\{2\}\$\$\,=\,0.74 and \$\$\{R\}\^\{2\}\$\$\,=\,0.70), and ejection fraction (\$\$\{R\}\^\{2\}\$\$\,=\,0.50), as well as predicted systemic phenotypes of age (\$\$\{R\}\^\{2\}\$\$\,=\,0.46), sex (AUC\,=\,0.88), weight (\$\$\{R\}\^\{2\}\$\$\,=\,0.56), and height (\$\$\{R\}\^\{2\}\$\$\,=\,0.33). Interpretation analysis validates that EchoNet shows appropriate attention to key cardiac structures when performing human-explainable tasks and highlights hypothesis-generating regions of interest when predicting systemic phenotypes difficult for human interpretation. Machine learning on echocardiography images can streamline repetitive tasks in the clinical workflow, provide preliminary interpretation in areas with insufficient qualified cardiologists, and predict phenotypes challenging for human evaluation.},
  langid = {english},
  keywords = {Cardiovascular diseases,Image processing,Machine learning}
}

@misc{GitLabCICD,
  title = {{{GitLab CI}}/{{CD}} | {{GitLab}}},
  abstract = {Learn how to use GitLab CI/CD, the GitLab built-in Continuous Integration, Continuous Deployment, and Continuous Delivery toolset to build, test, and deploy your application.},
  howpublished = {https://docs.gitlab.com/ee/ci/},
  langid = {american}
}

@misc{GNUEmacsGNU,
  title = {{{GNU Emacs}} - {{GNU Project}}},
  howpublished = {https://www.gnu.org/software/emacs/}
}

@misc{goldsteinPeekingBlackBox2014,
  title = {Peeking {{Inside}} the {{Black Box}}: {{Visualizing Statistical Learning}} with {{Plots}} of {{Individual Conditional Expectation}}},
  shorttitle = {Peeking {{Inside}} the {{Black Box}}},
  author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  year = {2014},
  month = mar,
  number = {arXiv:1309.6392},
  eprint = {1309.6392},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1309.6392},
  abstract = {This article presents Individual Conditional Expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features. In the presence of substantial interaction effects, the partial response relationship can be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate the complexity of the modeled relationship. Accordingly, ICE plots refine the partial dependence plot by graphing the functional relationship between the predicted response and the feature for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate, suggesting where and to what extent heterogeneities might exist. In addition to providing a plotting suite for exploratory analysis, we include a visual test for additive structure in the data generating model. Through simulated examples and real data sets, we demonstrate how ICE plots can shed light on estimated models in ways PDPs cannot. Procedures outlined are available in the R package ICEbox.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications}
}

@article{guidottiFactualCounterfactualExplanations2019,
  title = {Factual and {{Counterfactual Explanations}} for {{Black Box Decision Making}}},
  author = {Guidotti, Riccardo and Monreale, Anna and Giannotti, Fosca and Pedreschi, Dino and Ruggieri, Salvatore and Turini, Franco},
  year = {2019},
  month = nov,
  journal = {IEEE Intelligent Systems},
  volume = {34},
  number = {6},
  pages = {14--23},
  issn = {1941-1294},
  doi = {10.1109/MIS.2019.2957223},
  abstract = {The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.},
  eventtitle = {{{IEEE Intelligent Systems}}},
  keywords = {Counterfactuals,Data models,Decision making,Decision trees,Explainable AI,Explanation Rules,Genetic algorithms,Intelligent systems,Interpretable Machine Learning,Machine learning algorithms,Open the Black Box,Prediction algorithms}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning Data Mining}}, {{Inference}}, and {{Prediction}}, {{Second Edition}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009}
}

@article{hatwellAdaWHIPSExplainingAdaBoost2020,
  title = {Ada-{{WHIPS}}: {{Explaining AdaBoost}} Classification with Applications in the Health Sciences},
  shorttitle = {Ada-{{WHIPS}}},
  author = {Hatwell, Julian and Gaber, Mohamed Medhat and Atif Azad, R. Muhammad},
  year = {2020},
  month = oct,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {20},
  number = {1},
  pages = {250},
  issn = {1472-6947},
  doi = {10.1186/s12911-020-01201-2},
  abstract = {Computer Aided Diagnostics (CAD) can support medical practitioners to make critical decisions about their patients' disease conditions. Practitioners require access to the chain of reasoning behind CAD to build trust in the CAD advice and to supplement their own expertise. Yet, CAD systems might be based on black box machine learning models and high dimensional data sources such as electronic health records, magnetic resonance imaging scans, cardiotocograms, etc. These foundations make interpretation and explanation of the CAD advice very challenging. This challenge is recognised throughout the machine learning research community. eXplainable Artificial Intelligence (XAI) is emerging as one of the most important research areas of recent years because it addresses the interpretability and trust concerns of critical decision makers, including those in clinical and medical practice.},
  keywords = {AdaBoost,Black box problem,Computer aided diagnostics,Explainable AI,Interpretability}
}

@misc{hegselmannDevelopmentValidationInterpretable2021,
  title = {Development and {{Validation}} of an {{Interpretable}} 3-{{Day Intensive Care Unit Readmission Prediction Model Using Explainable Boosting Machines}}},
  author = {Hegselmann, Stefan and Ertmer, Christian and Volkert, Thomas and Gottschalk, Antje and Dugas, Martin and Varghese, Julian},
  year = {2021},
  month = nov,
  pages = {2021.11.01.21265700},
  publisher = {{medRxiv}},
  doi = {10.1101/2021.11.01.21265700},
  abstract = {Intensive care unit readmissions are associated with mortality and bad outcomes. Machine learning could help to identify patients at risk to improve discharge decisions. However, many models are black boxes, so that dangerous properties might remain unnoticed. In this study, an inherently interpretable model for 3-day ICU readmission prediction was developed. We used a retrospective cohort of 15,589 ICU stays and 169 variables collected between 2006 and 2019. A team of doctors inspected the model, checked the plausibility of each component, and removed problematic parts. Qualitative feedback revealed several challenges for interpretable machine learning in healthcare. The resulting model used 67 features and showed an area under the precision-recall curve of 0.119{$\pm$}0.020 and an area under the receiver operating characteristic curve of 0.680{$\pm$}0.025. This is on par with state-of-the-art gradient boosting machines and outperforms the Simplified Acute Physiology Score II. External validation with the Medical Information Mart for Intensive Care database version IV confirmed our findings. Hence, a machine learning model for readmission prediction with a high level of human control is feasible without sacrificing performance.},
  langid = {english}
}

@article{holzingerCausabilityExplainabilityArtificial2019,
  title = {Causability and Explainability of Artificial Intelligence in Medicine},
  author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and M{\"u}ller, Heimo},
  year = {2019},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {e1312},
  issn = {1942-4795},
  doi = {10.1002/widm.1312},
  abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Human Centricity and User Interaction},
  langid = {english},
  keywords = {artificial intelligence,causability,explainability,explainable AI,histopathology,medicine}
}

@article{holzingerMeasuringQualityExplanations2020,
  title = {Measuring the {{Quality}} of {{Explanations}}: {{The System Causability Scale}} ({{SCS}})},
  shorttitle = {Measuring the {{Quality}} of {{Explanations}}},
  author = {Holzinger, Andreas and Carrington, Andr{\'e} and M{\"u}ller, Heimo},
  year = {2020},
  month = jun,
  journal = {K\"unstl Intell},
  volume = {34},
  number = {2},
  pages = {193--198},
  issn = {1610-1987},
  doi = {10.1007/s13218-020-00636-z},
  abstract = {Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable AI (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable AI studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human\textendash AI interfaces for explainable AI. In order to build effective and efficient interactive human\textendash AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable AI system. In this paper we introduce our System Causability Scale to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al. in Wiley Interdiscip Rev Data Min Knowl Discov 9(4), 2019) combined with concepts adapted from a widely-accepted usability scale.},
  langid = {english},
  keywords = {Explainable AI,Humanâ€“AI interfaces,System causability scale (SCS)}
}

@misc{hookerUnrestrictedPermutationForces2021,
  title = {Unrestricted {{Permutation}} Forces {{Extrapolation}}: {{Variable Importance Requires}} at Least {{One More Model}}, or {{There Is No Free Variable Importance}}},
  shorttitle = {Unrestricted {{Permutation}} Forces {{Extrapolation}}},
  author = {Hooker, Giles and Mentch, Lucas and Zhou, Siyu},
  year = {2021},
  month = oct,
  number = {arXiv:1905.03151},
  eprint = {1905.03151},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.03151},
  abstract = {This paper reviews and advocates against the use of permute-and-predict (PaP) methods for interpreting black box functions. Methods such as the variable importance measures proposed for random forests, partial dependence plots, and individual conditional expectation plots remain popular because they are both model-agnostic and depend only on the pre-trained model output, making them computationally efficient and widely available in software. However, numerous studies have found that these tools can produce diagnostics that are highly misleading, particularly when there is strong dependence among features. The purpose of our work here is to (i) review this growing body of literature, (ii) provide further demonstrations of these drawbacks along with a detailed explanation as to why they occur, and (iii) advocate for alternative measures that involve additional modeling. In particular, we describe how breaking dependencies between features in hold-out data places undue emphasis on sparse regions of the feature space by forcing the original model to extrapolate to regions where there is little to no data. We explore these effects across various model setups and find support for previous claims in the literature that PaP metrics can vastly over-emphasize correlated features in both variable importance measures and partial dependence plots. As an alternative, we discuss and recommend more direct approaches that involve measuring the change in model performance after muting the effects of the features under investigation.},
  archiveprefix = {arXiv},
  keywords = {62G08,Computer Science - Machine Learning,I.5.1,Statistics - Machine Learning,Statistics - Methodology}
}

@article{huUsingMachineLearning2020,
  title = {Using a Machine Learning Approach to Predict Mortality in Critically Ill Influenza Patients: {{A}} Cross-Sectional Retrospective Multicentre Study in {{Taiwan}}},
  shorttitle = {Using a Machine Learning Approach to Predict Mortality in Critically Ill Influenza Patients},
  author = {Hu, Chien-An and Chen, Chia-Ming and Fang, Yen-Chun and Liang, Shinn-Jye and Wang, Hao-Chien and Fang, Wen-Feng and Sheu, Chau-Chyun and Perng, Wann-Cherng and Yang, Kuang-Yao and Kao, Kuo-Chin and Wu, Chieh-Liang and Tsai, Chwei-Shyong and Lin, Ming-Yen and Chao, Wen-Cheng},
  year = {2020},
  month = feb,
  journal = {BMJ Open},
  volume = {10},
  number = {2},
  eprint = {32102816},
  eprinttype = {pmid},
  pages = {e033898},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2019-033898},
  abstract = {Objectives Current mortality prediction models used in the intensive care unit (ICU) have a limited role for specific diseases such as influenza, and we aimed to establish an explainable machine learning (ML) model for predicting mortality in critically ill influenza patients using a real-world severe influenza data set. Study design A cross-sectional retrospective multicentre study in Taiwan Setting Eight medical centres in Taiwan. Participants A total of 336 patients requiring ICU-admission for virology-proven influenza at eight hospitals during an influenza epidemic between October 2015 and March 2016. Primary and secondary outcome measures We employed extreme gradient boosting (XGBoost) to establish the prediction model, compared the performance with logistic regression (LR) and random forest (RF), demonstrated the feature importance categorised by clinical domains, and used SHapley Additive exPlanations (SHAP) for visualised interpretation. Results The data set contained 76 features of the 336 patients with severe influenza. The severity was apparently high, as shown by the high Acute Physiology and Chronic Health Evaluation II score (22, 17 to 29) and pneumonia severity index score (118, 88 to 151). XGBoost model (area under the curve (AUC): 0.842; 95\% CI 0.749 to 0.928) outperformed RF (AUC: 0.809; 95\% CI 0.629 to 0.891) and LR (AUC: 0.701; 95\% CI 0.573 to 0.825) for predicting 30-day mortality. To give clinicians an intuitive understanding of feature exploitation, we stratified features by the clinical domain. The cumulative feature importance in the fluid balance domain, ventilation domain, laboratory data domain, demographic and symptom domain, management domain and severity score domain was 0.253, 0.113, 0.177, 0.140, 0.152 and 0.165, respectively. We further used SHAP plots to illustrate associations between features and 30-day mortality in critically ill influenza patients. Conclusions We used a real-world data set and applied an ML approach, mainly XGBoost, to establish a practical and explainable mortality prediction model in critically ill influenza patients.},
  langid = {english},
  keywords = {adult intensive \& critical care,infectious diseases \& infestations,information technology,thoracic medicine}
}

@article{islamSystematicReviewExplainable2022,
  title = {A {{Systematic Review}} of {{Explainable Artificial Intelligence}} in {{Terms}} of {{Different Application Domains}} and {{Tasks}}},
  author = {Islam, Mir Riyanul and Ahmed, Mobyen Uddin and Barua, Shaibal and Begum, Shahina},
  year = {2022},
  month = jan,
  journal = {Applied Sciences},
  volume = {12},
  number = {3},
  pages = {1353},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app12031353},
  abstract = {Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers' understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.},
  langid = {english},
  keywords = {evaluation metrics,explainability,explainable artificial intelligence,systematic literature review}
}

@article{jimenez-lunaDrugDiscoveryExplainable2020,
  title = {Drug Discovery with Explainable Artificial Intelligence},
  author = {{Jim{\'e}nez-Luna}, Jos{\'e} and Grisoni, Francesca and Schneider, Gisbert},
  year = {2020},
  month = oct,
  journal = {Nature machine intelligence},
  volume = {2},
  number = {10},
  pages = {573--584},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00236-4},
  abstract = {Deep learning bears promise for drug discovery, including advanced image analysis, prediction of molecular structure and function, and automated generation of innovative chemical entities with bespoke properties. Despite the growing number of successful prospective applications, the underlying mathematical models often remain elusive to interpretation by the human mind. There is a demand for `explainable' deep learning methods to address the need for a new narrative of the machine language of the molecular sciences. This Review summarizes the most prominent algorithmic concepts of explainable artificial intelligence, and forecasts future opportunities, potential applications as well as several remaining challenges. We also hope it encourages additional efforts towards the development and acceptance of explainable artificial intelligence techniques.},
  langid = {english},
  keywords = {Cheminformatics,Computational science,Drug discovery and development}
}

@article{JMLR:v12:pedregosa11a,
  title = {Scikit-Learn: {{Machine}} Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830}
}

@inproceedings{karimDeepCOVIDExplainerExplainableCOVID192020,
  title = {{{DeepCOVIDExplainer}}: {{Explainable COVID-19 Diagnosis}} from {{Chest X-ray Images}}},
  shorttitle = {{{DeepCOVIDExplainer}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Karim, Md. Rezaul and D{\"o}hmen, Till and Cochez, Michael and Beyan, Oya and {Rebholz-Schuhmann}, Dietrich and Decker, Stefan},
  year = {2020},
  month = dec,
  pages = {1034--1037},
  doi = {10.1109/BIBM49941.2020.9313304},
  abstract = {In this paper1, we proposed an explainable deep neural networks (DNN)-based method for automatic detection of COVID-19 symptoms from chest radiography (CXR) images, which we call `DeepCOVIDExplainer'. We used 15,959 CXR images of 15,854 patients, covering normal, pneumonia, and COVID-19 cases. CXR images are first comprehensively preprocessed and augmented before classifying with a neural ensemble method, followed by highlighting class-discriminating regions using gradient-guided class activation maps (Grad-CAM ++) and layer-wise relevance propagation (LRP). Further, we provide human-interpretable explanations for the diagnosis. Evaluation results show that our approach can identify COVID-19 cases with a positive predictive value (PPV) of 91.6\%, 92.45\%, and 96.12\%, respectively for normal, pneumonia, and COVID-19 cases, respectively, outperforming recent approaches.1Read longer version of this paper: https://arxiv.org/pdf/2004.04582.pdf},
  eventtitle = {2020 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  keywords = {Biomedical imaging,COVID-19,Deep learning,Explainability,Grad-CAM,Layer-wise relevance propagation,Lung,Predictive models,Reliability,Training,Visualization,X-ray imaging}
}

@misc{kennedyDevelopmentEnsembleMachine2021,
  title = {Development of an Ensemble Machine Learning Prognostic Model to Predict 60-{{Day}} Risk of Major Adverse Cardiac Events in Adults with Chest Pain},
  author = {Kennedy, Chris J. and Mark, Dustin G. and Huang, Jie and {van der Laan}, Mark J. and Hubbard, Alan E. and Reed, Mary E.},
  options = {useprefix=true},
  year = {2021},
  month = oct,
  pages = {2021.03.08.21252615},
  publisher = {{medRxiv}},
  doi = {10.1101/2021.03.08.21252615},
  abstract = {Background Chest pain is the second leading reason for emergency department (ED) visits and is commonly identified as a leading driver of low-value health care. Accurate identification of patients at low risk of major adverse cardiac events (MACE) is important to improve resource allocation and reduce over-treatment. Objectives We assessed machine learning (ML) methods and electronic health record (EHR) covariate collection for MACE prediction. We aimed to maximize the pool of low-risk patients that were accurately predicted to have less than 0.5\% MACE risk and could be eligible for reduced testing (``rule-out'' strategy). Population Studied 116,764 adult patients presenting with chest pain in the ED between 2013 and 2015 and evaluated for potential acute coronary syndrome (ACS). 60-day MACE rate was 2\%. Setting 21 emergency departments within the Kaiser Permanente Northern California integrated health system. Data analysis was performed May 2018 to August 2021. Methods We evaluated ML algorithms (lasso, splines, random forest, extreme gradient boosting, Bayesian additive regression trees) and SuperLearner stacked ensembling. We tuned ML hyperparameters through nested ensembling, and imputed missing values with generalized low-rank models (GLRM). Performance was benchmarked against individual biomarkers, validated clinical risk scores, decision trees, and logistic regression. We assessed clinical utility through net benefit analysis and explained the models through variable importance ranking and accumulated local effect plots. Results The SuperLearner ensemble provided the best cross-validated discrimination with areas under the curve of 0.15 for precision-recall (PR-AUC) and 0.87 for receiver operating characteristic (ROC-AUC), and the best accuracy with an index of prediction accuracy of 0.07. The ensemble's risk estimates were miscalibrated by 0.2 percentage points on average, and dominated the net benefit analysis at all examined thresholds. At a 0.5\% threshold the ensemble model yielded 32 benefit-adjusted workups avoided per 100 patients, compared to 25 for logistic regression and 2-14 for clinical risk scores. The most important predictors were age, troponin, clinical risk scores, and electrocardiogram. GLRM achieved a 90\% average reduction in reconstruction error compared to median-mode imputation. Conclusion Combining ML algorithms with a broad set of EHR covariates improved MACE risk prediction and would reduce over-treatment compared to simpler alternatives, while providing calibrated predictions and interpretability. Patients should receive targeted benefit in their care from thorough detection of nuanced health patterns via ML. The omission of prediction from the major goals of basic medical science has impoverished the intellectual content of clinical work, since a modern clinician's main challenge in the care of patients is to make predictions.Alvan Feinstein, 1983},
  langid = {english}
}

@article{kickbuschLancetFinancialTimes2021,
  title = {The {{Lancet}} and {{Financial Times Commission}} on Governing Health Futures 2030: {{Growing}} up in a Digital World},
  shorttitle = {The {{Lancet}} and {{Financial Times Commission}} on Governing Health Futures 2030},
  author = {Kickbusch, Ilona and Piselli, Dario and Agrawal, Anurag and Balicer, Ran and Banner, Olivia and Adelhardt, Michael and Capobianco, Emanuele and Fabian, Christopher and Singh Gill, Amandeep and Lupton, Deborah and Medhora, Rohinton P and Ndili, Njide and Ry{\'s}, Andrzej and Sambuli, Nanjira and Settle, Dykki and Swaminathan, Soumya and Morales, Jeanette Vega and Wolpert, Miranda and Wyckoff, Andrew W and Xue, Lan and Bytyqi, Aferdita and Franz, Christian and Gray, Whitney and Holly, Louise and Neumann, Micaela and Panda, Lipsa and Smith, Robert D and Georges Stevens, Enow Awah and Wong, Brian Li Han},
  year = {2021},
  month = nov,
  journal = {The Lancet},
  volume = {398},
  number = {10312},
  pages = {1727--1776},
  issn = {01406736},
  doi = {10.1016/S0140-6736(21)01824-9},
  langid = {english}
}

@article{klaiseAlibiExplainAlgorithms2021,
  title = {Alibi {{Explain}}: {{Algorithms}} for {{Explaining Machine Learning Models}}},
  shorttitle = {Alibi {{Explain}}},
  author = {Klaise, Janis and Looveren, Arnaud Van and Vacanti, Giovanni and Coca, Alexandru},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {181},
  pages = {1--7},
  issn = {1533-7928},
  abstract = {We introduce Alibi Explain, an open-source Python library for explaining predictions of machine learning models (https://github.com/SeldonIO/alibi). The library features state-of-the-art explainability algorithms for classification and regression models. The algorithms cover both the model-agnostic (black-box) and model-specific (white-box) setting, cater for multiple data types (tabular, text, images) and explanation scope (local and global explanations). The library exposes a unified API enabling users to work with explanations in a consistent way. Alibi adheres to best development practices featuring extensive testing of code correctness and algorithm convergence in a continuous integration environment. The library comes with extensive documentation of both usage and theoretical background of methods, and a suite of worked end-to-end use cases. Alibi aims to be a production-ready toolkit with integrations into machine learning deployment platforms such as Seldon Core and KFServing, and distributed explanation capabilities using Ray.}
}

@article{knuthLiterateProgramming1984,
  title = {Literate Programming},
  author = {Knuth, Donald E.},
  year = {1984},
  journal = {The Computer Journal},
  volume = {27},
  number = {2},
  pages = {97--111},
  keywords = {dblp}
}

@article{kongDiabetesItsComorbidities2013,
  title = {Diabetes and Its Comorbidities\textemdash Where {{East}} Meets {{West}}},
  author = {Kong, Alice P. S. and Xu, Gang and Brown, Nicola and So, Wing-Yee and Ma, Ronald C. W. and Chan, Juliana C. N.},
  year = {2013},
  month = sep,
  journal = {Nature Reviews Endocrinology},
  volume = {9},
  number = {9},
  pages = {537--547},
  publisher = {{Nature Publishing Group}},
  issn = {1759-5037},
  doi = {10.1038/nrendo.2013.102},
  abstract = {Alice Kong and her colleagues discuss how globalization and migration has changed diabetes profiles, focusing particularly on how lessons learnt in the East can be translated into clinical practice in the West.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Diabetes,Epidemiology}
}

@misc{lageEvaluationHumanInterpretabilityExplanation2019,
  title = {An {{Evaluation}} of the {{Human-Interpretability}} of {{Explanation}}},
  author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Sam and {Doshi-Velez}, Finale},
  year = {2019},
  month = aug,
  number = {arXiv:1902.00006},
  eprint = {1902.00006},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.00006},
  abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable under three specific tasks that users may perform with machine learning systems: simulation of the response, verification of a suggested response, and determining whether the correctness of a suggested response changes under a change to the inputs. Through carefully controlled human-subject experiments, we identify regularizers that can be used to optimize for the interpretability of machine learning systems. Our results show that the type of complexity matters: cognitive chunks (newly defined concepts) affect performance more than variable repetitions, and these trends are consistent across tasks and domains. This suggests that there may exist some common design principles for explanation systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{lakkarajuFaithfulCustomizableExplanations2019,
  title = {Faithful and {{Customizable Explanations}} of {{Black Box Models}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
  year = {2019},
  month = jan,
  series = {{{AIES}} '19},
  pages = {131--138},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3306618.3314229},
  abstract = {As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.},
  isbn = {978-1-4503-6324-2},
  keywords = {black box models,decision making,interpretable machine learning}
}

@article{lamyExplainableArtificialIntelligence2019,
  title = {Explainable Artificial Intelligence for Breast Cancer: {{A}} Visual Case-Based Reasoning Approach},
  shorttitle = {Explainable Artificial Intelligence for Breast Cancer},
  author = {Lamy, Jean-Baptiste and Sekar, Boomadevi and Guezennec, Gilles and Bouaud, Jacques and S{\'e}roussi, Brigitte},
  year = {2019},
  month = mar,
  journal = {Artificial Intelligence in Medicine},
  volume = {94},
  pages = {42--53},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2019.01.001},
  abstract = {Case-Based Reasoning (CBR) is a form of analogical reasoning in which the solution for a (new) query case is determined using a database of previous known cases with their solutions. Cases similar to the query are retrieved from the database, and then their solutions are adapted to the query. In medicine, a case usually corresponds to a patient and the problem consists of classifying the patient in a class of diagnostic or therapy. Compared to ``black box'' algorithms such as deep learning, the responses of CBR systems can be justified easily using the similar cases as examples. However, this possibility is often under-exploited and the explanations provided by most CBR systems are limited to the display of the similar cases. In this paper, we propose a CBR method that can be both executed automatically as an algorithm and presented visually in a user interface for providing visual explanations or for visual reasoning. After retrieving similar cases, a visual interface displays quantitative and qualitative similarities between the query and the similar cases, so as one can easily classify the query through visual reasoning, in a fully explainable manner. It combines a quantitative approach (visualized by a scatter plot based on Multidimensional Scaling in polar coordinates, preserving distances involving the query) and a qualitative approach (set visualization using rainbow boxes). We applied this method to breast cancer management. We showed on three public datasets that our qualitative method has a classification accuracy comparable to k-Nearest Neighbors algorithms, but is better explainable. We also tested the proposed interface during a small user study. Finally, we apply the proposed approach to a real dataset in breast cancer. Medical experts found the visual approach interesting as it explains why cases are similar through the visualization of shared patient characteristics.},
  langid = {english},
  keywords = {Breast cancer,Case-based reasoning,Data-driven decision making,Explainable Artificial Intelligence,Multidimensional Scaling,Visual explanation}
}

@article{lamyExplainableDecisionSupport2020,
  title = {Explainable Decision Support through the Learning and Visualization of Preferences from a Formal Ontology of Antibiotic Treatments},
  author = {Lamy, Jean-Baptiste and Sedki, Karima and Tsopra, Rosy},
  year = {2020},
  month = apr,
  journal = {Journal of Biomedical Informatics},
  volume = {104},
  pages = {103407},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2020.103407},
  abstract = {The aim of eXplainable Artificial Intelligence (XAI) is to design intelligent systems that can explain their predictions or recommendations to humans. Such systems are particularly desirable for therapeutic decision support, because physicians need to understand rcommendations to have confidence in their application and to adapt them if required, e.g. in case of patient contraindication. We propose here an explainable and visual approach for decision support in antibiotic treatment, based on an ontology. There were three steps to our method. We first generated a tabular dataset from the ontology, containing features defined on various domains and n-ary features. A preference model was then learned from patient profiles, antibiotic features and expert recommendations found in clinical practice guidelines. This model made the implicit rationale of the expert explicit, including the way in which missing data was treated. We then visualized the preference model and its application to all antibiotics available on the market for a given clinical situation, using rainbow boxes, a recently developed technique for set visualization. The resulting preference model had an error rate of 3.5\% on the learning data, and 5.2\% on test data (10-fold validation). These findings suggest that our system can help physicians to prescribe antibiotics correctly, even for clinical situations not present in the guidelines (e.g. due to allergies or contraindications for the recommended treatment).},
  langid = {english},
  keywords = {Antibiotics,Clinical decision support system,Explainable artificial intelligence,Ontologies,Preference learning,Preference visualization}
}

@article{lamyRainbowBoxesNew2017,
  title = {Rainbow Boxes: {{A}} New Technique for Overlapping Set Visualization and Two Applications in the Biomedical Domain},
  shorttitle = {Rainbow Boxes},
  author = {Lamy, Jean-Baptiste and Berthelot, H{\'e}l{\`e}ne and Capron, Coralie and Favre, Madeleine},
  year = {2017},
  month = dec,
  journal = {Journal of Visual Languages \& Computing},
  volume = {43},
  pages = {71--82},
  issn = {1045-926X},
  doi = {10.1016/j.jvlc.2017.09.003},
  abstract = {Overlapping set visualization is a well-known problem in information visualization. This problem considers elements and sets containing all or part of the elements, a given element possibly belonging to more than one set. A typical example is the properties of the 20 amino-acids. A more complex application is the visual comparison of the contraindications or the adverse effects of several similar drugs. The knowledge involved is voluminous, each drug has many contraindications and adverse effects, some of them are shared with other drugs. Another real-life application is the visualization of gene annotation, each gene product being annotated with several annotation terms indicating the associated biological processes, molecular functions and cellular components. In this paper, we present rainbow boxes, a novel technique for visualizing overlapping sets, and its application to the presentation of the properties of amino-acids, the comparison of drug properties, and the visualization of gene annotation. This technique requires solving a combinatorial optimization problem; we propose a specific heuristic and we evaluate and compare it to general optimization algorithms. We also describe a user study comparing rainbow boxes to tables and showing that the former allowed physicians to find information significantly faster. Finally, we discuss the limits and the perspectives of rainbow boxes.},
  langid = {english},
  keywords = {Drug properties,Gene annotation,Information visualization,Knowledge visualization,Overlapping set visualization,User study}
}

@article{longComorbiditiesDiabetesHypertension2011,
  title = {Comorbidities of {{Diabetes}} and {{Hypertension}}: {{Mechanisms}} and {{Approach}} to {{Target Organ Protection}}},
  shorttitle = {Comorbidities of {{Diabetes}} and {{Hypertension}}},
  author = {Long, Amanda N. and {Dagogo-Jack}, Samuel},
  year = {2011},
  journal = {The Journal of Clinical Hypertension},
  volume = {13},
  number = {4},
  pages = {244--251},
  issn = {1751-7176},
  doi = {10.1111/j.1751-7176.2011.00434.x},
  abstract = {Up to 75\% of adults with diabetes also have hypertension, and patients with hypertension alone often show evidence of insulin resistance. Thus, hypertension and diabetes are common, intertwined conditions that share a significant overlap in underlying risk factors (including ethnicity, familial, dyslipidemia, and lifestyle determinants) and complications. These complications include microvascular and macrovascular disorders. The macrovascular complications, which are well recognized in patients with longstanding diabetes or hypertension, include coronary artery disease, myocardial infarction, stroke, congestive heart failure, and peripheral vascular disease. Although microvascular complications (retinopathy, nephropathy, and neuropathy) are conventionally linked to hyperglycemia, studies have shown that hypertension constitutes an important risk factor, especially for nephropathy. The familial predisposition to diabetes and hypertension appears to be polygenic in origin, which militates against the feasibility of a ``gene therapy'' approach to the control or prevention of these conditions. On the other hand, the shared lifestyle factors in the etiology of hypertension and diabetes provide ample opportunity for nonpharmacologic intervention. Thus, the initial approach to the management of both diabetes and hypertension must emphasize weight control, physical activity, and dietary modification. Interestingly, lifestyle intervention is remarkably effective in the primary prevention of diabetes and hypertension. These principles also are pertinent to the prevention of downstream macrovascular complications of the two disorders. In addition to lifestyle modification, most patients will require specific medications to achieve national treatment goals for hypertension and diabetes. Management of hyperglycemia, hypertension, dyslipidemia, and the underlying hypercoagulable and proinflammatory states requires the use of multiple medications in combination. J Clin Hypertens (Greenwich). 2011;13:244\textendash 251. \textcopyright{} 2011 Wiley Periodicals, Inc.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-7176.2011.00434.x}
}

@inproceedings{louAccurateIntelligibleModels2013,
  title = {Accurate Intelligible Models with Pairwise Interactions},
  booktitle = {Proceedings of the 19th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},
  year = {2013},
  month = aug,
  series = {{{KDD}} '13},
  pages = {623--631},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2487575.2487579},
  abstract = {Standard generalized additive models (GAMs) usually model the dependent variable as a sum of univariate models. Although previous studies have shown that standard GAMs can be interpreted by users, their accuracy is significantly less than more complex models that permit interactions. In this paper, we suggest adding selected terms of interacting pairs of features to standard GAMs. The resulting models, which we call GA2\{M\}\$-models, for Generalized Additive Models plus Interactions, consist of univariate terms and a small number of pairwise interaction terms. Since these models only include one- and two-dimensional components, the components of GA2M-models can be visualized and interpreted by users. To explore the huge (quadratic) number of pairs of features, we develop a novel, computationally efficient method called FAST for ranking all possible pairs of features as candidates for inclusion into the model. In a large-scale empirical study, we show the effectiveness of FAST in ranking candidate pairs of features. In addition, we show the surprising result that GA2M-models have almost the same performance as the best full-complexity models on a number of real datasets. Thus this paper postulates that for many problems, GA2M-models can yield models that are both intelligible and accurate.},
  isbn = {978-1-4503-2174-7},
  keywords = {classification,interaction detection,regression}
}

@misc{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott and Lee, Su-In},
  year = {2017},
  month = nov,
  number = {arXiv:1705.07874},
  eprint = {1705.07874},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1705.07874},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@phdthesis{marvinQuantumLatticeLearning2022,
  type = {Thesis},
  title = {Quantum Lattice Learning and Explainable Artificial Intelligence for Maternal and Child Healthcare},
  author = {Marvin, Ggaliwango},
  year = {2022},
  abstract = {The current approach to maternal and child healthcare is extremely patient-centred,
 it requires costly, risky surveillance and testing before diagnosis besides treatment
 accompanied with uncertainty despite the essential combination of healthcare expertise,
 skills and experience in medical care and public health for medical practitioners
 to support maternal and child health.
 With the recent maternity and prenatal engagement besides the availability of health
 data and information, we interpretably revolutionize advances in maternal medicine
 by turning massive amounts of data into proactive, predictive, preventive, personalized
 and participatory optimal treatment plans through predictive and preventive
 medicine for maternal and child well being.
 This work focuses on interpretable predictive and Machine Learning (ML) modelling
 of Artificial Intelligence (AI) algorithms to be used in predictive analytics of
 health data for maternal precision medicine and explainable preventive insights for
 physicians and patients' medical decision making. We also introduced the concept
 of Quantum Lattice Learning for building Explainable Machine Learning models in
 Quantum Space.
 Due to the uncertainty caused by abstracted black-box AI and ML models (algorithms)
 used to support the maternal-child medical decisions, there is ambiguity
 of safety and trust of all the existing and proposed AI models. That hinders reliability
 and trust in adoption of the developed models by physicians and patients.
 We, therefore, implemented Explainable Artificial Intelligence (XAI) and feature
 interpretability analysis to allow clinicians like obstetricians, perinatologists, gynecologists
 and midwives to understandably trust, comprehensively assess connections
 and transparently analyze and use the important derived features for strategic maternal
 and child predictive, preventive and precision medicine.
 The adoption of the proposed XAI approaches (models) on health data usage could
 potentially strengthen health systems, public health, primary and surgical care for
 mothers and children globally. They can significantly improve accountability, reliability
 and adoption of safe and trusted artificial intelligence applications for improved
 maternal-fetal medicine besides global health. Moreover, our transparent models
 provide useful insights for healthcare management and policy-making to improve
 the health and well-being of patients and physicians.},
  copyright = {Brac University theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission.},
  langid = {english},
  school = {Brac University},
  annotation = {Accepted: 2022-05-25T03:29:57Z}
}

@article{millerExplanationArtificialIntelligence2019,
  title = {Explanation in Artificial Intelligence: {{Insights}} from the Social Sciences},
  shorttitle = {Explanation in Artificial Intelligence},
  author = {Miller, Tim},
  year = {2019},
  month = feb,
  journal = {Artificial Intelligence},
  volume = {267},
  pages = {1--38},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2018.07.007},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  langid = {english},
  keywords = {Explainability,Explainable AI,Explanation,Interpretability,Transparency}
}

@misc{ModelInterpretationSkater,
  title = {Model {{Interpretation}} with {{Skater}}},
  howpublished = {https://oracle.github.io/Skater/}
}

@book{molnarInterpretableMachineLearning2022,
  title = {Interpretable {{Machine Learning}}: {{A Guide For Making Black Box Models Explainable}}},
  shorttitle = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  year = {2022},
  month = feb,
  publisher = {{Independently published}},
  isbn = {9798411463330},
  langid = {english}
}

@book{molnarPermutationFeatureImportance2022,
  title = {8.5 {{Permutation Feature Importance}} | {{Interpretable Machine Learning}}: {{A Guide For Making Black Box Models Explainable}}. , 2022},
  author = {Molnar, Christoph},
  year = {2022},
  month = feb,
  publisher = {{Independently published}}
}

@inproceedings{moreno-sanchezDevelopmentExplainablePrediction2020,
  title = {Development of an {{Explainable Prediction Model}} of {{Heart Failure Survival}} by {{Using Ensemble Trees}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {{Moreno-Sanchez}, Pedro A.},
  year = {2020},
  month = dec,
  pages = {4902--4910},
  doi = {10.1109/BigData50022.2020.9378460},
  abstract = {Cardiovascular diseases (CVD) are the leading cause of death globally. Heart failure prediction, one of the CVD manifestations, has become a priority for doctors, however, up to date clinical practice usually has failed to reach high accuracy in such tasks. Machine learning offers advantages not only for clinical prediction but also for feature ranking improving the interpretation of the outputs by clinical professionals. Thus, the concept of eXplainable Artificial Intelligence (XAI) is aimed to cope with the lack of explainability of machine learning models in the healthcare domain, in this case, and provide healthcare professionals with patient-tailored decision-making tools that improve treatments and diagnostics. This paper presents a heart failure survival prediction model development by using ensemble trees machine learning techniques. Extreme Gradient Boosting (XGBoost) is demonstrated as the classifier with most accurate results (83\% accuracy with unseen data) over the other ensemble trees options. Moreover, a features selection preprocessing is made in order to assess which relevant features contribute to the model's results. Next, in terms of improving the explainability of the model developed, a study of features importance is carried out showing the "follow up time period" feature as the most relevant. Finally, a quantitative evaluation of the interpretability and fidelity of the model developed is performed obtaining a balanced ratio between these two indicators.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  keywords = {Big Data,Decision making,ensemble trees,explainable artificial intelligence,features importance,Heart,Heart failure survival prediction,machine learning,Medical services,Predictive models,Task analysis,Tools}
}

@article{naserDerivingMappingFunctions2022,
  title = {Deriving Mapping Functions to Tie Anthropometric Measurements to Body Mass Index via Interpretable Machine Learning},
  author = {Naser, M. Z.},
  year = {2022},
  month = jun,
  journal = {Machine Learning with Applications},
  volume = {8},
  pages = {100259},
  issn = {2666-8270},
  doi = {10.1016/j.mlwa.2022.100259},
  abstract = {This paper aims at leveraging recent advancements in interpretable machine learning to better understand how anthropometric measurements can be tied to body mass index (BMI). Two objectives are of interest to this work, the first is to develop a properly validated interpretable machine learning (ML) ensemble capable of accurately predicting BMI, and the second is to derive a mapping function (i.e., a ML-based expression) that can describe the relationship between BMI and anthropometric measurements. This paper analyzes a historical database published by Penrose et al. (1985) containing thirteen body circumference measurements for 252 men. Four ML algorithms are then blended into an ensemble to examine the collected anthropometric measurements, namely, Extreme Gradient Boosted Trees, Light Gradient Boosted Trees, Random Forest, and Keras Slim Residual Network. The ensemble was then augmented with the SHAP (SHapley Additive exPlanations) and partial dependence plot techniques to understand the effect of each anthropometric measurement on BMI and the interaction between these anthropometric measurements. The proposed ensemble not only can comprehend the relation between anthropometric measurements and BMI index and derive a new non-parametric expression to predict BMI, but can also be used to interpret such relation and help us understand the logic driving ML's predictions. Further, the interpretability analysis reveals that the main anthropometric measurements influencing BMI are the chest, abdomen, and hip circumference. Finally, clinicians and researchers are encouraged to leverage interpretability ML tools in their works instead of those of ``black-box'' nature.},
  langid = {english},
  keywords = {Anthropometric measurements,BMI,Interpretability,Machine learning}
}

@misc{noriInterpretMLUnifiedFramework2019,
  title = {{{InterpretML}}: {{A Unified Framework}} for {{Machine Learning Interpretability}}},
  shorttitle = {{{InterpretML}}},
  author = {Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},
  year = {2019},
  month = sep,
  number = {arXiv:1909.09223},
  eprint = {1909.09223},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.09223},
  abstract = {InterpretML is an open-source Python package which exposes machine learning interpretability algorithms to practitioners and researchers. InterpretML exposes two types of interpretability - glassbox models, which are machine learning models designed for interpretability (ex: linear models, rule lists, generalized additive models), and blackbox explainability techniques for explaining existing systems (ex: Partial Dependence, LIME). The package enables practitioners to easily compare interpretability algorithms by exposing multiple methods under a unified API, and by having a built-in, extensible visualization platform. InterpretML also includes the first implementation of the Explainable Boosting Machine, a powerful, interpretable, glassbox model that can be as accurate as many blackbox models. The MIT licensed source code can be downloaded from github.com/microsoft/interpret.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{ohExplainableMachineLearning2021,
  title = {Explainable {{Machine Learning Model}} for {{Glaucoma Diagnosis}} and {{Its Interpretation}}},
  author = {Oh, Sejong and Park, Yuli and Cho, Kyong Jin and Kim, Seong Jae},
  year = {2021},
  month = mar,
  journal = {Diagnostics},
  volume = {11},
  number = {3},
  pages = {510},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2075-4418},
  doi = {10.3390/diagnostics11030510},
  abstract = {The aim is to develop a machine learning prediction model for the diagnosis of glaucoma and an explanation system for a specific prediction. Clinical data of the patients based on a visual field test, a retinal nerve fiber layer optical coherence tomography (RNFL OCT) test, a general examination including an intraocular pressure (IOP) measurement, and fundus photography were provided for the feature selection process. Five selected features (variables) were used to develop a machine learning prediction model. The support vector machine, C5.0, random forest, and XGboost algorithms were tested for the prediction model. The performance of the prediction models was tested with 10-fold cross-validation. Statistical charts, such as gauge, radar, and Shapley Additive Explanations (SHAP), were used to explain the prediction case. All four models achieved similarly high diagnostic performance, with accuracy values ranging from 0.903 to 0.947. The XGboost model is the best model with an accuracy of 0.947, sensitivity of 0.941, specificity of 0.950, and AUC of 0.945. Three statistical charts were established to explain the prediction based on the characteristics of the XGboost model. Higher diagnostic performance was achieved with the XGboost model. These three statistical charts can help us understand why the machine learning model produces a specific prediction result. This may be the first attempt to apply ``explainable artificial intelligence'' to eye disease diagnosis.},
  langid = {english},
  keywords = {glaucoma,machine learning,model explanation,prediction}
}

@misc{OrgMode,
  title = {Org {{Mode}}},
  abstract = {Org-mode. Complex so you don't have to be. A versatile organisational system with immense capabilities.},
  howpublished = {https://orgmode.org},
  langid = {english}
}

@article{palatnikdesousaLocalInterpretableModelAgnostic2019,
  title = {Local {{Interpretable Model-Agnostic Explanations}} for {{Classification}} of {{Lymph Node Metastases}}},
  author = {{Palatnik de Sousa}, Iam and Maria Bernardes Rebuzzi Vellasco, Marley and {Costa da Silva}, Eduardo},
  year = {2019},
  month = jan,
  journal = {Sensors},
  volume = {19},
  number = {13},
  pages = {2969},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s19132969},
  abstract = {An application of explainable artificial intelligence on medical data is presented. There is an increasing demand in machine learning literature for such explainable models in health-related applications. This work aims to generate explanations on how a Convolutional Neural Network (CNN) detects tumor tissue in patches extracted from histology whole slide images. This is achieved using the ``locally-interpretable model-agnostic explanations'' methodology. Two publicly-available convolutional neural networks trained on the Patch Camelyon Benchmark are analyzed. Three common segmentation algorithms are compared for superpixel generation, and a fourth simpler parameter-free segmentation algorithm is proposed. The main characteristics of the explanations are discussed, as well as the key patterns identified in true positive predictions. The results are compared to medical annotations and literature and suggest that the CNN predictions follow at least some aspects of human expert knowledge.},
  langid = {english},
  keywords = {deep learning,explainable AI,lymph node metastases,medical data}
}

@inproceedings{paniguttiDoctorXAIOntologybased2020,
  title = {Doctor {{XAI}}: {{An}} Ontology-Based Approach to Black-Box Sequential Data Classification Explanations},
  shorttitle = {Doctor {{XAI}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Panigutti, Cecilia and Perotti, Alan and Pedreschi, Dino},
  year = {2020},
  month = jan,
  series = {{{FAT}}* '20},
  pages = {629--639},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3351095.3372855},
  abstract = {Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.},
  isbn = {978-1-4503-6936-7},
  keywords = {explainable artificial intelligence,healthcare data,machine learning}
}

@article{patExplainableMachineLearning2022,
  title = {Explainable Machine Learning Approach to Predict and Explain the Relationship between Task-Based {{fMRI}} and Individual Differences in Cognition},
  author = {Pat, Narun and Wang, Yue and Bartonicek, Adam and Candia, Juli{\'a}n and Stringaris, Argyris},
  year = {2022},
  month = jun,
  journal = {Cerebral Cortex},
  pages = {bhac235},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhac235},
  abstract = {Despite decades of costly research, we still cannot accurately predict individual differences in cognition from task-based functional magnetic resonance imaging (fMRI). Moreover, aiming for methods with higher prediction is not sufficient. To understand brain-cognition relationships, we need to explain how these methods draw brain information to make the prediction. Here we applied an explainable machine-learning (ML) framework to predict cognition from task-based fMRI during the n-back working-memory task, using data from the Adolescent Brain Cognitive Development (n =\,3,989). We compared 9 predictive algorithms in their ability to predict 12 cognitive abilities. We found better out-of-sample prediction from ML algorithms over the mass-univariate and ordinary least squares (OLS) multiple regression. Among ML algorithms, Elastic Net, a linear and additive algorithm, performed either similar to or better than nonlinear and interactive algorithms. We explained how these algorithms drew information, using SHapley Additive explanation, eNetXplorer, Accumulated Local Effects, and Friedman's H-statistic. These explainers demonstrated benefits of ML over the OLS multiple regression. For example, ML provided some consistency in variable importance with a previous study and consistency with the mass-univariate approach in the directionality of brain-cognition relationships at different regions. Accordingly, our explainable-ML framework predicted cognition from task-based fMRI with boosted prediction and explainability over standard methodologies.}
}

@article{payrovnaziriExplainableArtificialIntelligence2020,
  title = {Explainable Artificial Intelligence Models Using Real-World Electronic Health Record Data: {{A}} Systematic Scoping Review},
  shorttitle = {Explainable Artificial Intelligence Models Using Real-World Electronic Health Record Data},
  author = {Payrovnaziri, Seyedeh Neelufar and Chen, Zhaoyi and {Rengifo-Moreno}, Pablo and Miller, Tim and Bian, Jiang and Chen, Jonathan H and Liu, Xiuwen and He, Zhe},
  year = {2020},
  month = jul,
  journal = {Journal of the American Medical Informatics Association},
  volume = {27},
  number = {7},
  pages = {1173--1185},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocaa053},
  abstract = {To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions.We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges.Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (N\,=\,13), intrinsically interpretable models (N\,=\,9), data dimensionality reduction (N\,=\,8), attention mechanism (N\,=\,7), and feature interaction and importance (N\,=\,5).XAI evaluation is an open issue that requires a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view.Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.}
}

@misc{PDPbox,
  title = {{{PDPbox}} 0.2.0},
  howpublished = {https://pdpbox.readthedocs.io/en/latest/}
}

@article{phillipsFourPrinciplesExplainable2021,
  title = {Four {{Principles}} of {{Explainable Artificial Intelligence}}},
  author = {Phillips, P. Jonathon and Hahn, Carina and Fontana, Peter and Yates, Amy and Greene, Kristen K. and Broniatowski, David A. and Przybocki, Mark A.},
  year = {2021},
  month = sep,
  journal = {NIST},
  publisher = {{P. Jonathon Phillips, Carina Hahn, Peter Fontana, Amy Yates, Kristen K. Greene, David A. Broniatowski, Mark A. Przybocki}},
  langid = {english}
}

@article{pinascoInterpretableMachineLearning2022,
  title = {An Interpretable Machine Learning Model for Covid-19 Screening},
  author = {Pinasco, Gustavo Carreiro and Farina, Eduardo Moreno J{\'u}dice de Mattos and Filho, Fabiano Novaes Barcellos and Fiorotti, Willer Fran{\c c}a and Ferreira, Matheus Coradini Mariano and Cruz, Sheila Cristina de Souza and Colodette, Andre Louzada and Loureiro, Luciene Rossati and Com{\'e}rio, Tatiane and Farias, Dilzilene Cunha Sivirino and Lima, Eliane de F{\'a}tima Almeida and Manhambusque, Katia Val{\'e}ria},
  year = {2022},
  month = jun,
  journal = {Journal of Human Growth and Development},
  volume = {32},
  number = {2},
  pages = {268--274},
  issn = {2175-3598},
  doi = {10.36311/jhgd.v32.13324},
  abstract = {Introduction: the Coronavirus Disease 2019 (COVID-19) is a viral disease which has been declared a pandemic by the WHO. Diagnostic tests are expensive and are not always available. Researches using machine learning (ML) approach for diagnosing SARS-CoV-2 infection have been proposed in the literature to reduce cost and allow better control of the pandemic. Objective: we aim to develop a machine learning model to predict if a patient has COVID-19 with epidemiological data and clinical features. Methods: we used six ML algorithms for COVID-19 screening through diagnostic prediction and did an interpretative analysis using SHAP models and feature importances. Results: our best model was XGBoost (XGB) which obtained an area under the ROC curve of 0.752, a sensitivity of 90\%, a specificity of 40\%, a positive predictive value (PPV) of 42.16\%, and a negative predictive value (NPV) of 91.0\%. The best predictors were fever, cough, history of international travel less than 14 days ago, male gender, and nasal congestion, respectively. Conclusion: We conclude that ML is an important tool for screening with high sensitivity, compared to rapid tests, and can be used to empower clinical precision in COVID-19, a disease in which symptoms are very unspecific.},
  langid = {english},
  keywords = {artificial intelligence,COVID-19,machine learning,pandemia}
}

@article{portoMinimumRelevantFeatures2021,
  title = {Minimum {{Relevant Features}} to {{Obtain Explainable Systems}} for {{Predicting Cardiovascular Disease Using}} the {{Statlog Data Set}}},
  author = {Porto, Roberto and Molina, Jos{\'e} M. and Berlanga, Antonio and Patricio, Miguel A.},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {3},
  pages = {1285},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app11031285},
  abstract = {Learning systems have been focused on creating models capable of obtaining the best results in error metrics. Recently, the focus has shifted to improvement in the interpretation and explanation of the results. The need for interpretation is greater when these models are used to support decision making. In some areas, this becomes an indispensable requirement, such as in medicine. The goal of this study was to define a simple process to construct a system that could be easily interpreted based on two principles: (1) reduction of attributes without degrading the performance of the prediction systems and (2) selecting a technique to interpret the final prediction system. To describe this process, we selected a problem, predicting cardiovascular disease, by analyzing the well-known Statlog (Heart) data set from the University of California's Automated Learning Repository. We analyzed the cost of making predictions easier to interpret by reducing the number of features that explain the classification of health status versus the cost in accuracy. We performed an analysis on a large set of classification techniques and performance metrics, demonstrating that it is possible to construct explainable and reliable models that provide high quality predictive performance.},
  langid = {english},
  keywords = {cardiovascular disease prediction,healthcare,interpretable artificial intelligence,machine learning}
}

@misc{ProjectJupyter,
  title = {Project {{Jupyter}}},
  abstract = {The Jupyter Notebook is a web-based interactive computing platform. The notebook combines live code, equations, narrative text, visualizations, interactive dashboards and other media.},
  howpublished = {https://jupyter.org},
  langid = {english}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  series = {{{KDD}} '16},
  pages = {1135--1144},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2939672.2939778},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  isbn = {978-1-4503-4232-2},
  keywords = {black box classifier,explaining machine learning,interpretability,interpretable machine learning}
}

@incollection{robnik-sikonjaPerturbationBasedExplanationsPrediction2018,
  title = {Perturbation-{{Based Explanations}} of {{Prediction Models}}},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  author = {{Robnik-{\v S}ikonja}, Marko and Bohanec, Marko},
  editor = {Zhou, Jianlong and Chen, Fang},
  year = {2018},
  series = {Human\textendash{{Computer Interaction Series}}},
  pages = {159--175},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-90403-0_9},
  abstract = {Current research into algorithmic explanation methods for predictive models can be divided into two main approaches: gradient-based approaches limited to neural networks and more general perturbation-based approaches which can be used with arbitrary prediction models. We present an overview of perturbation-based approaches, with focus on the most popular methods (EXPLAIN, IME, LIME). These methods support explanation of individual predictions but can also visualize the model as a whole. We describe their working principles, how they handle computational complexity, their visualizations as well as their advantages and disadvantages. We illustrate practical issues and challenges in applying the explanation methodology in a business context on a practical use case of B2B sales forecasting in a company. We demonstrate how explanations can be used as a what-if analysis tool to answer relevant business questions.},
  isbn = {978-3-319-90403-0},
  langid = {english}
}

@inproceedings{saricaExplainableBoostingMachine2021,
  title = {Explainable {{Boosting Machine}} for {{Predicting Alzheimer}}'s {{Disease}} from {{MRI Hippocampal Subfields}}},
  booktitle = {Brain {{Informatics}}},
  author = {Sarica, Alessia and Quattrone, Andrea and Quattrone, Aldo},
  editor = {Mahmud, Mufti and Kaiser, M. Shamim and Vassanelli, Stefano and Dai, Qionghai and Zhong, Ning},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {341--350},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-86993-9_31},
  abstract = {Although automatic prediction of Alzheimer's disease (AD) from Magnetic Resonance Imaging (MRI) showed excellent performance, Machine Learning (ML) algorithms often provide high accuracy at the expense of interpretability of findings. Indeed, building ML models that can be understandable has fundamental importance in clinical context, especially for early diagnosis of neurodegenerative diseases. Recently, a novel interpretability algorithm has been proposed, the Explainable Boosting Machine (EBM), which is a glassbox model based on Generative Additive Models plus Interactions GA2Ms and designed to show optimal accuracy while providing intelligibility. Thus, the aim of present study was to assess \textendash{} for the first time \textendash{} the EBM reliability in predicting the conversion to AD and its ability in providing the predictions explainability. In particular, two-hundred brain MRIs from ADNI of Mild Cognitive Impairment (MCI) patients equally divided into stable (sMCI) and progressive (pMCI) were processed with Freesurfer for extracting twelve hippocampal subfields volumes, which already showed good AD prediction power. EBM models with and without pairwise interactions were built on training set (80\%) comprised of these volumes, and global explanations were investigated. The performance of classifiers was evaluated with AUC-ROC on test set (20\%) and local explanations of four randomly selected test patients (sMCIs and pMCIs correctly classified and misclassified) were given. EBMs without and with pairwise interactions showed accuracies of respectively 80.5\% and 84.2\%, thus demonstrating high prediction accuracy. Moreover, EBM provided practical clinical knowledge on why a patient was correctly or incorrectly predicted as AD and which hippocampal subfields drove such prediction.},
  isbn = {978-3-030-86993-9},
  langid = {english},
  keywords = {Alzheimerâ€™s disease,Explainable boosting machine,Hippocampal subfields,MRI}
}

@inproceedings{saukkonen2021human,
  title = {Human Rights, Employee Rights and Copyrights: {{Parallels}} of {{AI}} Enablers and Obstacles across Occupations in Human-Centric Domains},
  booktitle = {Proceedings of the 3rd European Conference on the Impact of Artificial Intelligence and Robotics {{ECIAIR}} 2021, {{A}} Virtual Conference Hosted by {{Iscte}}\textendash{{Instituto}} Universit\'ario de Lisboa 18-19 November 2021},
  author = {Saukkonen, Juha and Anton, Kirill and Karpov, Nikita and Lahti, William},
  year = {2021},
  pages = {166--173}
}

@misc{schwalbeComprehensiveTaxonomyExplainable2022,
  title = {A {{Comprehensive Taxonomy}} for {{Explainable Artificial Intelligence}}: {{A Systematic Survey}} of {{Surveys}} on {{Methods}} and {{Concepts}}},
  shorttitle = {A {{Comprehensive Taxonomy}} for {{Explainable Artificial Intelligence}}},
  author = {Schwalbe, Gesina and Finzel, Bettina},
  year = {2022},
  month = may,
  number = {arXiv:2105.07190},
  eprint = {2105.07190},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.07190},
  abstract = {In the meantime, a wide variety of terminologies, motivations, approaches, and evaluation criteria have been developed within the research field of explainable artificial intelligence (XAI). With the amount of XAI methods vastly growing, a taxonomy of methods is needed by researchers as well as practitioners: To grasp the breadth of the topic, compare methods, and to select the right XAI method based on traits required by a specific use-case context. Many taxonomies for XAI methods of varying level of detail and depth can be found in the literature. While they often have a different focus, they also exhibit many points of overlap. This paper unifies these efforts and provides a complete taxonomy of XAI methods with respect to notions present in the current state of research. In a structured literature analysis and meta-study, we identified and reviewed more than 50 of the most cited and current surveys on XAI methods, metrics, and method traits. After summarizing them in a survey of surveys, we merge terminologies and concepts of the articles into a unified structured taxonomy. Single concepts therein are illustrated by more than 50 diverse selected example methods in total, which we categorize accordingly. The taxonomy may serve both beginners, researchers, and practitioners as a reference and wide-ranging overview of XAI method traits and aspects. Hence, it provides foundations for targeted, use-case-oriented, and context-sensitive future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0,I.2.6,I.2.m}
}

@article{senatoreAutomaticDiagnosisNeurodegenerative2019,
  title = {Automatic {{Diagnosis}} of {{Neurodegenerative Diseases}}: {{An Evolutionary Approach}} for {{Facing}} the {{Interpretability Problem}}},
  shorttitle = {Automatic {{Diagnosis}} of {{Neurodegenerative Diseases}}},
  author = {Senatore, Rosa and Della Cioppa, Antonio and Marcelli, Angelo},
  year = {2019},
  month = jan,
  journal = {Information-an International Interdisciplinary Journal},
  volume = {10},
  number = {1},
  pages = {30},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2078-2489},
  doi = {10.3390/info10010030},
  abstract = {Background: The use of Artificial Intelligence (AI) systems for automatic diagnoses is increasingly in the clinical field, being a useful support for the identification of several diseases. Nonetheless, the acceptance of AI-based diagnoses by the physicians is hampered by the black-box approach implemented by most performing systems, which do not clearly state the classification rules adopted. Methods: In this framework we propose a classification method based on a Cartesian Genetic Programming (CGP) approach, which allows for the automatic identification of the presence of the disease, and concurrently, provides the explicit classification model used by the system. Results: The proposed approach has been evaluated on the publicly available HandPD dataset, which contains handwriting samples drawn by Parkinson's disease patients and healthy controls. We show that our approach compares favorably with state-of-the-art methods, and more importantly, allows the physician to identify an explicit model relevant for the diagnosis based on the most informative subset of features. Conclusion: The obtained results suggest that the proposed approach is particularly appealing in that, starting from the explicit model, it allows the physicians to derive a set of guidelines for defining novel testing protocols and intervention strategies.},
  langid = {english},
  keywords = {E-health,evolutionary computation,explainable artificial intelligence,machine learning,Parkinson disease}
}

@inproceedings{shankaranarayanaALIMEAutoencoderBased2019,
  title = {{{ALIME}}: {{Autoencoder Based Approach}} for {{Local Interpretability}}},
  shorttitle = {{{ALIME}}},
  booktitle = {Intelligent {{Data Engineering}} and {{Automated Learning}} \textendash{} {{IDEAL}} 2019},
  author = {Shankaranarayana, Sharath M. and Runje, Davor},
  editor = {Yin, Hujun and Camacho, David and Tino, Peter and {Tall{\'o}n-Ballesteros}, Antonio J. and Menezes, Ronaldo and Allmendinger, Richard},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {454--463},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-33607-3_49},
  abstract = {Machine learning and especially deep learning have garnered tremendous popularity in recent years due to their increased performance over other methods. The availability of large amount of data has aided in the progress of deep learning. Nevertheless, deep learning models are opaque and often seen as black boxes. Thus, there is an inherent need to make the models interpretable, especially so in the medical domain. In this work, we propose a locally interpretable method, which is inspired by one of the recent tools that has gained a lot of interest, called local interpretable model-agnostic explanations (LIME). LIME generates single instance level explanation by artificially generating a dataset around the instance (by randomly sampling and using perturbations) and then training a local linear interpretable model. One of the major issues in LIME is the instability in the generated explanation, which is caused due to the randomly generated dataset. Another issue in these kind of local interpretable models is the local fidelity. We propose novel modifications to LIME by employing an autoencoder, which serves as a better weighting function for the local model. We perform extensive comparisons with different datasets and show that our proposed method results in both improved stability, as well as local fidelity.},
  isbn = {978-3-030-33607-3},
  langid = {english},
  keywords = {Autoencoder,Deep learning,Explainable AI (XAI),Healthcare,Interpretable machine learning}
}

@article{shiExplainableMachineLearning2022,
  title = {Explainable Machine Learning Model for Predicting the Occurrence of Postoperative Malnutrition in Children with Congenital Heart Disease},
  author = {Shi, Hui and Yang, Dong and Tang, Kaichen and Hu, Chunmei and Li, Lijuan and Zhang, Linfang and Gong, Ting and Cui, Yanqin},
  year = {2022},
  month = jan,
  journal = {Clinical Nutrition},
  volume = {41},
  number = {1},
  eprint = {34906845},
  eprinttype = {pmid},
  pages = {202--210},
  publisher = {{Elsevier}},
  issn = {0261-5614, 1532-1983},
  doi = {10.1016/j.clnu.2021.11.006},
  langid = {english},
  keywords = {Children,Congenital heart disease,Interpretability,Machine learning,Malnutrition}
}

@article{singhImodelsPythonPackage2021,
  title = {Imodels: A Python Package for Fitting Interpretable Models},
  shorttitle = {Imodels},
  author = {Singh, Chandan and Nasseri, Keyan and Tan, Yan and Tang, Tiffany and Yu, Bin},
  year = {2021},
  month = may,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {61},
  pages = {3192},
  issn = {2475-9066},
  doi = {10.21105/joss.03192}
}

@article{staniakExplanationsModelPredictions2018,
  title = {Explanations of Model Predictions with Live and {{breakDown}} Packages},
  author = {Staniak, Mateusz and Biecek, Przemyslaw},
  year = {2018},
  month = apr,
  doi = {10.32614/RJ-2018-072},
  abstract = {Complex models are commonly used in predictive modeling. In this paper we present R packages that can be used to explain predictions from complex black box models and attribute parts of these predictions to input features. We introduce two new approaches and corresponding packages for such attribution, namely live and breakDown. We also compare their results with existing implementations of state of the art solutions, namely lime that implements Locally Interpretable Model-agnostic Explanations and ShapleyR that implements Shapley values.},
  langid = {english}
}

@article{stepinSurveyContrastiveCounterfactual2021,
  title = {A {{Survey}} of {{Contrastive}} and {{Counterfactual Explanation Generation Methods}} for {{Explainable Artificial Intelligence}}},
  author = {Stepin, Ilia and Alonso, Jose M. and Catala, Alejandro and {Pereira-Fari{\~n}a}, Mart{\'i}n},
  year = {2021},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {9},
  pages = {11974--12001},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3051315},
  abstract = {A number of algorithms in the field of artificial intelligence offer poorly interpretable decisions. To disclose the reasoning behind such algorithms, their output can be explained by means of so-called evidence-based (or factual) explanations. Alternatively, contrastive and counterfactual explanations justify why the output of the algorithms is not any different and how it could be changed, respectively. It is of crucial importance to bridge the gap between theoretical approaches to contrastive and counterfactual explanation and the corresponding computational frameworks. In this work we conduct a systematic literature review which provides readers with a thorough and reproducible analysis of the interdisciplinary research field under study. We first examine theoretical foundations of contrastive and counterfactual accounts of explanation. Then, we report the state-of-the-art computational frameworks for contrastive and counterfactual explanation generation. In addition, we analyze how grounded such frameworks are on the insights from the inspected theoretical approaches. As a result, we highlight a variety of properties of the approaches under study and reveal a number of shortcomings thereof. Moreover, we define a taxonomy regarding both theoretical and practical approaches to contrastive and counterfactual explanation.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Artificial intelligence,Cognition,Computational intelligence,contrastive explanations,counterfactuals,explainable artificial intelligence,Signal to noise ratio,systematic literature review,Systematics,Taxonomy,Terminology,Training}
}

@misc{visaniOptiLIMEOptimizedLIME2022,
  title = {{{OptiLIME}}: {{Optimized LIME Explanations}} for {{Diagnostic Computer Algorithms}}},
  shorttitle = {{{OptiLIME}}},
  author = {Visani, Giorgio and Bagli, Enrico and Chesani, Federico},
  year = {2022},
  month = feb,
  number = {arXiv:2006.05714},
  eprint = {2006.05714},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.05714},
  abstract = {Local Interpretable Model-Agnostic Explanations (LIME) is a popular method to perform interpretability of any kind of Machine Learning (ML) model. It explains one ML prediction at a time, by learning a simple linear model around the prediction. The model is trained on randomly generated data points, sampled from the training dataset distribution and weighted according to the distance from the reference point - the one being explained by LIME. Feature selection is applied to keep only the most important variables. LIME is widespread across different domains, although its instability - a single prediction may obtain different explanations - is one of the major shortcomings. This is due to the randomness in the sampling step, as well as to the flexibility in tuning the weights and determines a lack of reliability in the retrieved explanations, making LIME adoption problematic. In Medicine especially, clinical professionals trust is mandatory to determine the acceptance of an explainable algorithm, considering the importance of the decisions at stake and the related legal issues. In this paper, we highlight a trade-off between explanation's stability and adherence, namely how much it resembles the ML model. Exploiting our innovative discovery, we propose a framework to maximise stability, while retaining a predefined level of adherence. OptiLIME provides freedom to choose the best adherence-stability trade-off level and more importantly, it clearly highlights the mathematical properties of the retrieved explanation. As a result, the practitioner is provided with tools to decide whether the explanation is reliable, according to the problem at hand. We extensively test OptiLIME on a toy dataset - to present visually the geometrical findings - and a medical dataset. In the latter, we show how the method comes up with meaningful explanations both from a medical and mathematical standpoint.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{wangPrevalenceTreatmentDiabetes2021,
  title = {Prevalence and {{Treatment}} of {{Diabetes}} in {{China}}, 2013-2018},
  author = {Wang, Limin and Peng, Wen and Zhao, Zhenping and Zhang, Mei and Shi, Zumin and Song, Ziwei and Zhang, Xiao and Li, Chun and Huang, Zhengjing and Sun, Xiaomin and Wang, Linhong and Zhou, Maigeng and Wu, Jing and Wang, Youfa},
  year = {2021},
  month = dec,
  journal = {JAMA},
  volume = {326},
  number = {24},
  pages = {2498--2506},
  issn = {0098-7484},
  doi = {10.1001/jama.2021.22208},
  abstract = {Recent data on prevalence, awareness, treatment, and risk factors of diabetes in China is necessary for interventional efforts.To estimate trends in prevalence, awareness, treatment, and risk factors of diabetes in China based on national data.Cross-sectional nationally representative survey data collected in adults aged 18 years or older in mainland China from 170\,287 participants in the 2013-2014 years and 173\,642 participants in the 2018-2019 years.Fasting plasma glucose and hemoglobin A1c levels were measured for all participants. A 2-hour oral glucose tolerance test was conducted for all participants without diagnosed diabetes.Primary outcomes were diabetes and prediabetes defined according to American Diabetes Association criteria. Secondary outcomes were awareness, treatment, and control of diabetes and prevalence of risk factors. A hemoglobin A1c level of less than 7.0\% (53 mmol/mol) among treated patients with diabetes was considered adequate glycemic control.In 2013, the median age was 55.8 years (IQR, 46.4-65.2 years) and the weighted proportion of women was 50.0\%; in 2018, the median age was 51.3 years (IQR, 42.1-61.6 years), and the weighted proportion of women was 49.5\%. The estimated prevalence of diabetes increased from 10.9\% (95\% CI, 10.4\%-11.5\%) in 2013 to 12.4\% (95\% CI, 11.8\%-13.0\%) in 2018 (P\,\&lt;\,.001). The estimated prevalence of prediabetes was 35.7\% (95\% CI, 34.2\%-37.3\%) in 2013 and 38.1\% (95\% CI, 36.4\%-39.7\%) in 2018 (P\,=\,.07). In 2018, among adults with diabetes, 36.7\% (95\% CI, 34.7\%-38.6\%) reported being aware of their condition, and 32.9\% (95\% CI, 30.9\%-34.8\%) reported being treated; 50.1\% (95\% CI, 47.5\%-52.6\%) of patients receiving treatment were controlled adequately. These rates did not change significantly from 2013. From 2013 to 2018, low physical activity, high intake of red meat, overweight, and obesity significantly increased in prevalence.In this survey study, the estimated diabetes prevalence was high and increased from 2013 to 2018. There was no significant improvement in the estimated prevalence of adequate treatment.}
}

@misc{WHODiabetesFact,
  title = {{{WHO}} - {{Diabetes}} Fact Sheets},
  abstract = {Diabetes is a chronic disease that occurs either when the pancreas does not produce enough insulin or when the body cannot effectively use the insulin it produces.},
  howpublished = {https://www.who.int/news-room/fact-sheets/detail/diabetes},
  langid = {english}
}

@article{wuInterpretableMachineLearning2021,
  title = {Interpretable {{Machine Learning}} for {{COVID-19}}: {{An Empirical Study}} on {{Severity Prediction Task}}},
  shorttitle = {Interpretable {{Machine Learning}} for {{COVID-19}}},
  author = {Wu, Han and Ruan, Wenjie and Wang, Jiangtao and Zheng, Dingchang and Liu, Bei and Geng, Yayuan and Chai, Xiangfei and Chen, Jian and Li, Kunwei and Li, Shaolin and Helal, Sumi},
  year = {2021},
  journal = {IEEE Transactions on Artificial Intelligence},
  pages = {1--1},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3092698},
  abstract = {The black-box nature of machine learning models hinders the deployment of some high-accuracy models in medical diagnosis. It is risky to put one's life in the hands of models that medical researchers do not fully understand. However, through model interpretation, black-box models can promptly reveal significant biomarkers that medical practitioners may have overlooked due to the surge of infected patients in the COVID-19 pandemic. This research leverages a database of 92 patients with confirmed SARS-CoV-2 laboratory tests between 18th Jan. 2020 and 5th Mar. 2020, in Zhuhai, China, to identify biomarkers indicative of severity prediction. Through the interpretation of four machine learning models, decision tree, random forests, gradient boosted trees, and neural networks using permutation feature importance, Partial Dependence Plot (PDP), Individual Conditional Expectation (ICE), Accumulated Local Effects (ALE), Local Interpretable Model-agnostic Explanations (LIME), and Shapley Additive Explanation (SHAP), we identify an increase in N-Terminal pro-Brain Natriuretic Peptide (NTproBNP), C-Reaction Protein (CRP), and lactic dehydrogenase (LDH), a decrease in lymphocyte (LYM) is associated with severe infection and an increased risk of death, which is consistent with recent medical research on COVID-19 and other research using dedicated models. We further validate our methods on a large open dataset with 5644 confirmed patients from the Hospital Israelita Albert Einstein, at So Paulo, Brazil from Kaggle, and unveil leukocytes, eosinophils, and platelets as three indicative biomarkers for COVID-19.},
  eventtitle = {{{IEEE Transactions}} on {{Artificial Intelligence}}},
  keywords = {Biological system modeling,COVID-19,Machine learning,Medical diagnostic imaging,Medical services,Pandemics,Predictive models}
}

@misc{yangWhoDiesCOVID192020,
  title = {Who Dies from {{COVID-19}}? {{Post-hoc}} Explanations of Mortality Prediction Models Using Coalitional Game Theory, Surrogate Trees, and Partial Dependence Plots},
  shorttitle = {Who Dies from {{COVID-19}}?},
  author = {Yang, Russell},
  year = {2020},
  month = jun,
  pages = {2020.06.07.20124933},
  publisher = {{medRxiv}},
  doi = {10.1101/2020.06.07.20124933},
  abstract = {As of early June, 2020, approximately 7 million COVID-19 cases and 400,000 deaths have been reported. This paper examines four demographic and clinical factors (age, time to hospital, presence of chronic disease, and sex) and utilizes Shapley values from coalitional game theory and machine learning to evaluate their relative importance in predicting COVID-19 mortality. The analyses suggest that out of the 4 factors studied, age is the most important in predicting COVID-19 mortality, followed by time to hospital. Sex and presence of chronic disease were both found to be relatively unimportant, and the two global interpretation techniques differed in ranking them. Additionally, this paper creates partial dependence plots to determine and visualize the marginal effect of each factor on COVID-19 mortality and demonstrates how local interpretation of COVID-19 mortality prediction can be applicable in a clinical setting. Lastly, this paper derives clinically applicable decision rules about mortality probabilities through a parsimonious 3-split surrogate tree, demonstrating that high-accuracy COVID-19 mortality prediction can be achieved with simple, interpretable models.},
  langid = {english}
}

@article{yanInterpretableMortalityPrediction2020,
  title = {An Interpretable Mortality Prediction Model for {{COVID-19}} Patients},
  author = {Yan, Li and Zhang, Hai-Tao and Goncalves, Jorge and Xiao, Yang and Wang, Maolin and Guo, Yuqi and Sun, Chuan and Tang, Xiuchuan and Jing, Liang and Zhang, Mingyang and Huang, Xiang and Xiao, Ying and Cao, Haosen and Chen, Yanyan and Ren, Tongxin and Wang, Fang and Xiao, Yaru and Huang, Sufang and Tan, Xi and Huang, Niannian and Jiao, Bo and Cheng, Cheng and Zhang, Yong and Luo, Ailin and Mombaerts, Laurent and Jin, Junyang and Cao, Zhiguo and Li, Shusheng and Xu, Hui and Yuan, Ye},
  year = {2020},
  month = may,
  journal = {Nature machine intelligence},
  volume = {2},
  number = {5},
  pages = {283--288},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0180-7},
  abstract = {The sudden increase in COVID-19 cases is putting high pressure on healthcare services worldwide. At this stage, fast, accurate and early clinical assessment of the disease severity is vital. To support decision making and logistical planning in healthcare systems, this study leverages a database of blood samples from 485 infected patients in the region of Wuhan, China, to identify crucial predictive biomarkers of disease mortality. For this purpose, machine learning tools selected three biomarkers that predict the mortality of individual patients more than 10 days in advance with more than 90\% accuracy: lactic dehydrogenase (LDH), lymphocyte and high-sensitivity C-reactive protein (hs-CRP). In particular, relatively high levels of LDH alone seem to play a crucial role in distinguishing the vast majority of cases that require immediate medical attention. This finding is consistent with current medical knowledge that high LDH levels are associated with tissue breakdown occurring in various diseases, including pulmonary disorders such as pneumonia. Overall, this Article suggests a simple and operable decision rule to quickly predict patients at the highest risk, allowing them to be prioritized and potentially reducing the mortality rate.},
  langid = {english},
  keywords = {Applied mathematics,Diagnosis,Predictive markers,SARS-CoV-2,Software}
}

@article{yuArtificialIntelligenceHealthcare2018,
  title = {Artificial Intelligence in Healthcare},
  author = {Yu, Kun-Hsing and Beam, Andrew L. and Kohane, Isaac S.},
  year = {2018},
  month = oct,
  journal = {Nature biomedical engineering},
  volume = {2},
  number = {10},
  pages = {719--731},
  publisher = {{Nature Publishing Group}},
  issn = {2157-846X},
  doi = {10.1038/s41551-018-0305-z},
  abstract = {Artificial intelligence (AI) is gradually changing medical practice. With recent progress in digitized data acquisition, machine learning and computing infrastructure, AI applications are expanding into areas that were previously thought to be only the province of human experts. In this Review Article, we outline recent breakthroughs in AI technologies and their biomedical applications, identify the challenges for further progress in medical AI systems, and summarize the economic, legal and social implications of AI in healthcare.},
  langid = {english},
  keywords = {Computational models,Machine learning,Predictive medicine}
}

@article{zhangInterpretableDeepLearning2021,
  title = {Interpretable Deep Learning for Automatic Diagnosis of 12-{{Lead}} Electrocardiogram},
  author = {Zhang, Dongdong and Yang, Samuel and Yuan, Xiaohui and Zhang, Ping},
  year = {2021},
  month = apr,
  journal = {iScience},
  volume = {24},
  number = {4},
  pages = {102373},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2021.102373},
  abstract = {Electrocardiogram (ECG) is a widely used reliable, non-invasive approach for cardiovascular disease diagnosis. With the rapid growth of ECG examinations and the insufficiency of cardiologists, accurate and automatic diagnosis of ECG signals has become a hot research topic. In this paper, we developed a deep neural network for automatic classification of cardiac arrhythmias from 12-lead ECG recordings. Experiments on a public 12-lead ECG dataset showed the effectiveness of our method. The proposed model achieved an average F1 score of 0.813. The deep model showed superior performance than 4 machine learning methods learned from extracted expert features. Besides, the deep models trained on single-lead ECGs produce lower performance than using all 12 leads simultaneously. The best-performing leads are lead I, aVR, and V5 among 12 leads. Finally, we employed the SHapley Additive exPlanations method to interpret the model's behavior at both the patient level and population level.},
  langid = {english},
  keywords = {Artificial Intelligence,Clinical Finding,Medicine}
}
